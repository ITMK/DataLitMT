{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![KeyVisual_DataLitMT - Kopie.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "# Data Planning and Data Collection – Advanced Level\n",
        "**How to select, download, prepare and process data for training NMT models**"
      ],
      "metadata": {
        "id": "N6X_uoKQOe-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "9OMqy2eeOubs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab Notebook is concerned with the *Data Planning and Data Collection (& Production)* dimensions of the [DataLit<sup>MT</sup> Competence Matrix](https://itmk.github.io/The-DataLitMT-Project/matrix/). The notebook guides you step-by-step through gathering, preparing and processing data in order to train a neural machine translation (NMT) model, using, for example the [OpenNMT-py toolkit](https://opennmt.net/OpenNMT-py/). These steps are the central preparatory steps of an MT training pipeline, which is covered under *Technical MT Literacy* in the [Professional Machine Translation Literacy Framework](https://itmk.github.io/The-DataLitMT-Project/framework/#professional-mt-literacy). The code sections of this notebook are based on [Yasmin Moslem's repository for MT data preparation](https://github.com/ymoslem/MT-Preparation). You might also be interested in [this TAUS article on data cleaning for natural language processing](https://www.taus.net/resources/blog/ten-step-guide-to-data-cleaning), which highlights the various steps of a data cleaning pipeline. The first five steps discussed in this article are also covered in this notebook.\n",
        "\n",
        "**General Information**\n",
        "\n",
        "NMT models are trained on large datasets in the form of bilingual text corpora, which are aligned at sentence-level. NMT models are very sensitive to defects (also called 'noise') in their training data, as described, for example, in [Khayrallah/Koehn (2018)](https://aclanthology.org/W18-2709/). Therefore, thoroughly preparing the training datasets (cleaning, filtering, aligning, tokenizing, etc.) is paramount for creating high-quality NMT models.\n",
        "\n",
        "**Note:** To avoid losing your data produced throughout this notebook, do **not** close this window until you have completed all steps and downloaded the datasets locally or saved them to your Google Drive! This step is illustrated at the end of this notebook.\n",
        "\n",
        "A walk-through of this notebook and more detailed information on gathering and preparing data and the detailed steps performed in the code cells is provided in the accompanying tutorial video.\n",
        "\n",
        "---\n",
        "**Steps to take**\n",
        "\n",
        "As outlined in our DataLit<sup>MT</sup> Competence Matrix, a number of steps have to be taken to acquire data for training an NMT model. A **data requirement analysis** will show you which bilingual (also called parallel) data will be needed for training an NMT model that suits your individual requirements (domain, language combination, etc.). For more information on how to create custom machine translation models, see the chapter by [Ramírez-Sánchez (2022)](https://zenodo.org/record/6760022/files/342-Kenny-2022-9.pdf). Based on your data requirement analysis, you can then develop a **data strategy**, which will outline the steps required to obtain the data. Taking into account relevant aspects of **data curation and protection**, you can then identify and evaluate suitable sources for downloading bilingual datasets. For example, one source of publicly available parallel datasets, which can be used for training NMT models, is the [OPUS corpus collection](https://opus.nlpl.eu). Most datasets in the OPUS collection are available for non-commercial (research) use and can be searched by simply selecting a source and a target language. Once you have found a suitable dataset, you can **verify** this data by checking the type of dataset (web-crawled etc.) and clicking on *sample* to check the quality of the data. For MT, [moses](http://www2.statmt.org/moses/index.php?n=Main.HomePage) is used as the preferred download format, as the sentences are already aligned (meaning that every line in the source file refers to that same line in the target file, i.e., these lines are translations of each other). You can then **acquire** the data by clicking on the *moses* field or by running the cell below (when you right-click on the *moses* field of the dataset you selected on the OPUS website, you can copy the link to download the dataset. This is also shown in the accompanying tutorial video).\n",
        "\n",
        "For our current MT project scenario, we choose to work with the English-German parallel dataset *TED2020*, where we chose English as the source and German as the target language. When running the cell below, a .zip file will be downloaded and the two files we're interested in are those that end on the language codes: `.en` and `.de`. The data we prepare and save in this notebook can then be used to train an NMT model from scratch using our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing).\n",
        "\n",
        "Note: TED2020 is a comparatively small dataset for NMT training, containing 'only' 300,000 parallel sentences (more information on this dataset can be found [here](https://opus.nlpl.eu/TED2020.php)). This is sufficient to train a demo NMT model and to go through all steps required. Note, however, that datasets for training more efficient and high-performing NMT models, usually contain millions of sentences.\n",
        "\n",
        "Run the following cell to download the dataset from the OPUS collection."
      ],
      "metadata": {
        "id": "5ys_Heb-OxKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2mDd6NL7mHlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8cfa11-c2fb-445a-a925-732c5be25504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-27 11:35:35--  https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/de-en.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21857900 (21M) [application/zip]\n",
            "Saving to: ‘de-en.txt.zip’\n",
            "\n",
            "de-en.txt.zip       100%[===================>]  20.84M  10.9MB/s    in 1.9s    \n",
            "\n",
            "2023-02-27 11:35:38 (10.9 MB/s) - ‘de-en.txt.zip’ saved [21857900/21857900]\n",
            "\n",
            "Archive:  de-en.txt.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: TED2020.de-en.de        \n",
            "  inflating: TED2020.de-en.en        \n",
            "  inflating: TED2020.de-en.xml       \n"
          ]
        }
      ],
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/de-en.txt.zip\n",
        "!unzip de-en.txt.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should give you the two monolingual files:\n",
        "\n",
        "*   `TED2020.de-en.en`\n",
        "*   `TED2020.de-en.de`\n",
        "\n",
        "Note: we will not be working with the third file, i.e., the xml file.\n",
        "\n",
        "Each file has a sentence/segment per line that is a matching (aligned) translation of the same line in the other file (this is called [parallel text format](https://google.github.io/seq2seq/nmt/)).\n",
        "\n",
        "Note: If you wish to download a different dataset, just change the URL in the code cell above (make sure, that the data is still in *moses* format)."
      ],
      "metadata": {
        "id": "AL8JY1DOO1TP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "KYg2zaQ8O3rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we move on to filtering, **preparing and processing** this dataset. At the end of the notebook, we will show you how to download the resulting datasets to a local drive or how to save them to your Google Drive. After making sure that the data intended to train an NMT model is a bilingual dataset aligned at sentence-level, we can take different steps to clean this MT training data. These steps are outlined below."
      ],
      "metadata": {
        "id": "_RamM2VgO5xp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKn8NgtAjb6_"
      },
      "source": [
        "**Installing Necessary Library-Requirements**\n",
        "\n",
        "First, we'll install a range of necessary python libraries which are required for our data preparation. To do so, simply run the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur-AigWEla2D"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sacremoses\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lUSUmE_htODb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKxPsiwFqpBT"
      },
      "source": [
        "**Defining Datasets and Languages**\n",
        "\n",
        "Now, we'll define our source file and language (source dataset, English) as well as our target file and language (target dataset, German) based on the *TED2020* dataset we downloaded in the previous step. Simply run the cell below. From now on, we'll work with the variables `source_file`, `target_file`, `source_lang` and `target_lang`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wyg9wXL7nGob"
      },
      "outputs": [],
      "source": [
        "source_file = \"TED2020.de-en.en\"\n",
        "target_file = \"TED2020.de-en.de\"\n",
        "source_lang = \"en\"\n",
        "target_lang = \"de\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you have downloaded a different dataset, simply change the names and languages (marked in red) in the strings above to match your data."
      ],
      "metadata": {
        "id": "HFXtmKWmPMOY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b-2vCHxnGRI"
      },
      "source": [
        "**Filtering & Cleaning Datasets for MT Training**\n",
        "\n",
        "Now we move on to actually filtering and cleaning the datasets, as required for MT training purposes. This step will take the two source and target files and delete rows that contain empty cells, delete duplicates, source-copied rows and source/target sentences that are too long, remove HTML code, list how many rows will remain in true-cased form, delete rows with empty cells, shuffle rows and finally save source and target files (= filtered dataset). You can find more information on these data preparation steps in section 3 of [Bui et al. (2020)](https://aclanthology.org/2020.eamt-1.35). The final files should be adequately filtered and cleaned for NMT training purposes. \n",
        "\n",
        "The separate steps of cleaning and filtereing the source and target files are shown below. You do **not** need to change any parts of the code, simply run the cells and check out the different data preparation stages.\n",
        "\n",
        "For a more compact version of preparing your data, check out our [basic-level notebook](https://colab.research.google.com/drive/17aCfPF0Zw80gW0FYg_iRo16YVSMAaNKG?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmF2VhPuDaV"
      },
      "source": [
        "1. Run the cell below to create a [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7XA4MorguC_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb9de4e-352c-4e4f-9b14-d5e59206794e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape (rows, columns): (296354, 2)\n"
          ]
        }
      ],
      "source": [
        "# display(df) works only if you are in IPython/Jupyter Notebooks or enable:\n",
        "#from IPython.display import display\n",
        "\n",
        "df_source = pd.read_csv(source_file, names=['Source'], sep=\"\\n\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, on_bad_lines=\"skip\")\n",
        "df_target = pd.read_csv(target_file, names=['Target'], sep=\"\\n\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, on_bad_lines=\"skip\")\n",
        "df = pd.concat([df_source, df_target], axis=1)  # Join the two dataframes along columns\n",
        "\n",
        "print(\"Dataframe shape (rows, columns):\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPvleRfDtxXW"
      },
      "source": [
        "2. Run the cell below to delete rows with empty cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j3aoQR8btLcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d4a08f-6b86-4b54-e107-9fec8503c2df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with Empty Cells Deleted --> Rows: 293645\n"
          ]
        }
      ],
      "source": [
        "df = df.dropna()\n",
        "\n",
        "print(\"Rows with Empty Cells Deleted --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHe2TbYqtz8i"
      },
      "source": [
        "3. Run the cell below to delete duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8tURWz9NtsKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106e593d-4b62-44fd-e41e-1d9378710501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates Deleted --> Rows: 289373\n"
          ]
        }
      ],
      "source": [
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"Duplicates Deleted --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zzpmj0duJ-g"
      },
      "source": [
        "4. Run the cell below to delete source-copied rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rM4tbinuMkI"
      },
      "outputs": [],
      "source": [
        "df[\"Source-Copied\"] = df['Source'] == df['Target']\n",
        "df = df.set_index(['Source-Copied'])\n",
        "\n",
        "try: # To avoid (KeyError: '[True] not found in axis') if there are no source-copied cells\n",
        "    df = df.drop([True]) # Boolean, not string, do not add quotes\n",
        "except:\n",
        "    pass\n",
        "    \n",
        "df = df.reset_index()\n",
        "df = df.drop(['Source-Copied'], axis = 1)\n",
        "    \n",
        "print(\"Source-Copied Rows Deleted --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cHqMnqjuisJ"
      },
      "source": [
        "5. Run the cell below to delete source/target sentences that are too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "knZKSgbJuu3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9269f0-9f03-40be-a99f-e041ce6b7d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Too Long Source/Target Deleted --> Rows: 286843\n"
          ]
        }
      ],
      "source": [
        "df[\"Too-Long\"] = ((df['Source'].str.count(' ')+1) > (df['Target'].str.count(' ')+1) * 2) |  \\\n",
        "                 ((df['Target'].str.count(' ')+1) > (df['Source'].str.count(' ')+1) * 2) |  \\\n",
        "                 ((df['Source'].str.count(' ')+1) > 200) |  \\\n",
        "                 ((df['Target'].str.count(' ')+1) > 200)\n",
        "                \n",
        "df = df.set_index(['Too-Long'])\n",
        "\n",
        "try: # To avoid (KeyError: '[True] not found in axis') if there are no too-long cells\n",
        "    df = df.drop([True]) # Boolean, not string, do not add quotes\n",
        "except:\n",
        "    pass\n",
        "\n",
        "df = df.reset_index()\n",
        "df = df.drop(['Too-Long'], axis = 1)\n",
        "\n",
        "print(\"Too Long Source/Target Deleted --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7ibVGwQvARh"
      },
      "source": [
        "6. Run the cell below to remove HTML code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4dXU6HcAvCzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6272e9f-1dc6-4b54-d25d-fce8d5eab82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML Removed --> Rows: 286843\n"
          ]
        }
      ],
      "source": [
        "# Use str() to avoid (TypeError: expected string or bytes-like object)\n",
        "# Note: removing tags should be before removing empty cells because some cells might have only tags and become empty.\n",
        "\n",
        "df = df.replace(r'<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|{}', ' ', regex=True)\n",
        "df = df.replace(r'  ', ' ', regex=True)  # replace double-spaces with one space\n",
        "\n",
        "print(\"HTML Removed --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9CR031EvDJT"
      },
      "source": [
        "7. Run the cell below to list how many rows will remain in true-cased form (truecasing is an NLP problem of finding the proper capitalisation of words within a text where such information is unavailable; for more information, see [this article](https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eJ6FhGWBvF8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc496724-9ea3-4c2d-ab01-c921f1e1e3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows will remain in true-cased --> Rows: 286843\n"
          ]
        }
      ],
      "source": [
        "lower=False\n",
        "if lower == True:\n",
        "  df['Source'] = df['Source'].str.lower()\n",
        "  df['Target'] = df['Target'].str.lower()\n",
        "\n",
        "  print(\"Rows are now lower-cased --> Rows:\", df.shape[0])\n",
        "else:\n",
        "  print(\"Rows will remain in true-cased --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANd6gLDDvJbp"
      },
      "source": [
        "8. Run the cell below to replace empty cells with [`NaN`](https://pandas.pydata.org/docs/user_guide/missing_data.html) and delete `NaN` (either already there, or generated from the previous steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xQ8zQ1sUvKB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a876a36-ae84-4c40-8471-f936b1546567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with Empty Cells (Nan) Deleted --> Rows: 286843\n"
          ]
        }
      ],
      "source": [
        "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Rows with Empty Cells (Nan) Deleted --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C80s2nNdvMIt"
      },
      "source": [
        "9. Run the cell below to shuffle rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mRR53awqvMoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0849748-b98a-45de-8099-266884f1f4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows Shuffled --> Rows: 286843\n"
          ]
        }
      ],
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(\"Rows Shuffled --> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sYJn70XwMBQ"
      },
      "source": [
        "**Preparing to save the data**\n",
        "10. Run the cell below to write the dataframe to two filtered and cleaned source and target files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N6Si6CAlwK5Q"
      },
      "outputs": [],
      "source": [
        "source_filtered = source_file+'-filtered.'+source_lang\n",
        "target_filtered = target_file+'-filtered.'+target_lang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQnbl2aCwSfV"
      },
      "source": [
        "11. Run the cell below to save the filtered and cleaned source and target files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yMsvQGbpwRAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af9f3bb-0567-439a-ff68-585d88c9c3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Source Saved: TED2020.de-en.en-filtered.en\n",
            "Filtered Target Saved: TED2020.de-en.de-filtered.de\n"
          ]
        }
      ],
      "source": [
        "df_source = df[\"Source\"]\n",
        "df_target = df[\"Target\"]\n",
        "\n",
        "df_source.to_csv(source_filtered, header=False, index=False, quoting=csv.QUOTE_NONE, sep=\"\\n\")\n",
        "print(\"Filtered Source Saved:\", source_filtered)\n",
        "\n",
        "df_target.to_csv(target_filtered, header=False, index=False, quoting=csv.QUOTE_NONE, sep=\"\\n\")\n",
        "print(\"Filtered Target Saved:\", target_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final output, you should now have the two filtered and cleaned source and target files:\n",
        "\n",
        "*  source data: `TED2020.de-en.en-filtered.en`\n",
        "*  target data: `TED2020.de-en.de-filtered.de`\n",
        "\n"
      ],
      "metadata": {
        "id": "8fvMDwpMPygC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxaG10rO9K-x"
      },
      "source": [
        "**Checking the Dataset** \n",
        "\n",
        "It is always good practice to check your dataset in between steps to make sure that everything has worked well. For example, you could check that sentences are properly aligned. Run the cell below to print the first and last two lines of the filtered and cleaned source and target files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_X-WZGV89J2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5895e914-6fa8-4230-c076-97eef05f929d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 Lines:\n",
            "He said that he had been in the Gulf a little while ago; like about a week ago, and a guy who had been a recreational fishing guide took him out to show him what's going on. \n",
            "On the other side of the country, I went to class and wrote a poem in the margins of my notebook. \n",
            "----\n",
            "Er erzaehlte mir, dass er vor kurzem am Golf gewesen sei, -- vor etwa einer Woche -- und ein Mann, der als Tourguide fuer Freizeitangler gearbeitet hatte, nahm ihn mit, um ihm zu zeigen, was los war. \n",
            "Auf der anderen Seite des Landes ging ich zur Schule und schrieb ein Gedicht an meinen Heftrand. \n",
            "\n",
            "Last 2 Lines:\n",
            "It's just love. That's all it is. \n",
            "Now, why are algorithms even important? \n",
            "-----\n",
            "Es ist Liebe. Das ist alles. \n",
            "Warum sind Algorithmen überhaupt wichtig? \n"
          ]
        }
      ],
      "source": [
        "print(\"First 2 Lines:\")\n",
        "!head -n 2 'TED2020.de-en.en-filtered.en' && echo \"----\" && head -n 2 'TED2020.de-en.de-filtered.de'\n",
        "print()\n",
        "print(\"Last 2 Lines:\")\n",
        "!tail -n 2 'TED2020.de-en.en-filtered.en' && echo \"-----\" && tail -n 2 'TED2020.de-en.de-filtered.de'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Subword) Tokenizing with SentencePiece"
      ],
      "metadata": {
        "id": "EcQsdUv4P4he"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qnzrOHE2Stn"
      },
      "source": [
        "For the next step of our data preparation pipeline, we will tokenize the previously filtered datasets. This is a necessary step for MT training. Unlike humans, a computer does not read text in the form of individual words and sentences, but rather as a continuous string of characters (blank spaces included). Therefore, text (vocabulary) used to train MT models (and other Natural Language Processing or NLP models) must be tokenized so that the models know where the boundaries between individual words (and sentences) are.\n",
        "\n",
        "A special form of tokenization is subwording, where words are split into smaller units (so-called subwords) from which complete words can be assembled. The subword tokenization method is described in more detail in [Sennrich et al. (2016)](https://aclanthology.org/P16-1162/). NMT models are usually trained with such subwords instead of whole words in order to reduce the size of the NMT model’s vocabulary (the larger the vocabulary, the higher the data processing requirements will be). For example, the four words *low*, *lowest*, *high* and *highest* can be represented by the three subwords *low*, *high* and *est*, which would reduce the required vocabulary size of the NMT model by 1. During translation, when the MT model comes across a new word/token that resembles a word/token that is also present in its training vocabulary (or can be assembled from the subwords in this vocabulary), the MT model will be able to translate this word/token. Otherwise, it would produce an “unk” (= “unknown”) token as output. If you would like to know more about (subword) tokenization for training NLP models, have a look at [this article](https://blog.floydhub.com/tokenization-nlp/).\n",
        "\n",
        "A preferred toolkit for subword tokenization is [SentencePiece](https://github.com/google/sentencepiece). To apply SentencePiece, a subwording model must first be trained on the source and target files of the dataset. This model can then be applied to these files in order to subword them. These subworded source and target files are then used in the MT training process. When the trained model is used for translating, it will produce a subworded translation. Using the same SentencePiece model, this subworded translation must then be “desubworded” or “decoded” back into fluent text. Note that in systems such as DeepL or Google Translate, these subwording and desubwording steps are performed \"backstage\" so that we as users are not aware of them.\n",
        "\n",
        "Follow the steps below (again, do not change any code, simply run the cells) to train a SentencePiece model for subword tokenization based on the two previously filtered files: `TED2020.de-en.en-filtered.en` and `TED2020.de-en.de-filtered.de`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUWymi55xZm_"
      },
      "source": [
        "First, run the cell below to import further requirements for SentencePiece."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G9PIoA9nxZO7"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U12wJD5HxrD2"
      },
      "source": [
        "Run the cell below to create paths for the filtered and cleaned source and target files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7DtQix1Cxqxn"
      },
      "outputs": [],
      "source": [
        "path = \"\"    # change the path if needed\n",
        "\n",
        "train_source_file_tok = path + source_filtered\n",
        "train_target_file_tok = path + target_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cQKc77ZxjTo"
      },
      "source": [
        "Now we'll train SentencePiece models from the source and target files and create `source/target.model` and `source/target.vocab`. `source/target.vocab` is just a reference, not used in the segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWXt1t8uxzKz"
      },
      "source": [
        "**Training the source subword model**\n",
        "\n",
        "Run the cell below to train a SentencePiece subwording model on the filtered source file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GZRVfyN6wvGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c05979e1-795f-4ea5-f9d5-6e214f2ab9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, training a SentencepPiece model for the Source finished successfully!\n"
          ]
        }
      ],
      "source": [
        "# Source subword model\n",
        "\n",
        "source_train_value = '--input='+train_source_file_tok+' --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true'\n",
        "spm.SentencePieceTrainer.train(source_train_value)\n",
        "print(\"Done, training a SentencepPiece model for the Source finished successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG1W2C_1yMOy"
      },
      "source": [
        "**Training the target subword model**\n",
        "\n",
        "Run the cell below to train a SentencePiece subwording model on the filtered target file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TdTuOH_xyL6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807e40b4-d7e2-4d14-92b9-896559e0ae78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ],
      "source": [
        "# Target subword model\n",
        "\n",
        "target_train_value = '--input='+train_target_file_tok+' --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true'\n",
        "spm.SentencePieceTrainer.train(target_train_value)\n",
        "print(\"Done, training a SentencepPiece model for the Target finished successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two separate SentencePiece subwording models have now been trained: `source.model` and `target.model`. At the end of this notebook, we will also save the subworded source and target model as these will be needed for future subwording and desubwording (the subwording and desubwording step is also required in the [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing))."
      ],
      "metadata": {
        "id": "ICzZ443GQX4I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWvajFHNNQ51"
      },
      "source": [
        "**Subwording the filtered source and target datasets**\n",
        "\n",
        "By applying these source and target SentencePiece models, the previously filtered files can now be subworded. To do so, run the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8TzPnd3Pi_7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbb1f11-e01f-4ddd-a8b8-5d28d6a52ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Filtered Source Dataset: TED2020.de-en.en-filtered.en\n",
            "Filtered Target Dataset: TED2020.de-en.de-filtered.de\n"
          ]
        }
      ],
      "source": [
        "source_model = 'source.model'\n",
        "target_model = 'target.model'\n",
        "source_subworded = source_filtered + \".subword\"\n",
        "target_subworded = target_filtered + \".subword\"\n",
        "\n",
        "print(\"Source Model:\", source_model)\n",
        "print(\"Target Model:\", target_model)\n",
        "print(\"Filtered Source Dataset:\", source_filtered)\n",
        "print(\"Filtered Target Dataset:\", target_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yKcAMoIZ_K-F"
      },
      "outputs": [],
      "source": [
        "sp = spm.SentencePieceProcessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subwording the source dataset**\n",
        "\n",
        "Run the cell below to apply the source SentencePiece model on the filtered source file `TED2020.de-en.en-filtered.en`."
      ],
      "metadata": {
        "id": "FxGgqhgDRAWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BrrI0Mtc_Jxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e598907-1a6c-4190-af2b-aa7e4d8c8989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done subwording the source file! Output: TED2020.de-en.en-filtered.en.subword\n"
          ]
        }
      ],
      "source": [
        "sp.load(source_model)\n",
        "\n",
        "with open(source_filtered) as source, open(source_subworded, \"w+\") as source_subword:\n",
        "    for line in source:\n",
        "        line = line.strip()\n",
        "        line = sp.encode_as_pieces(line)\n",
        "        # line = ['<s>'] + line + ['</s>']    # add start & end tokens; optional\n",
        "        line = \" \".join([token for token in line])\n",
        "        source_subword.write(line + \"\\n\")\n",
        "\n",
        "print(\"Done subwording the source file! Output:\", source_subworded)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subwording the target dataset**\n",
        "\n",
        "Run the cell below to apply the target SentencePiece model on the filtered target file `TED2020.de-en.de-filtered.de`."
      ],
      "metadata": {
        "id": "LcJekCCCRNVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hRq1E8vVjN9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84c1c20-ad14-4e70-db1a-d467cf2c95e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done subwording the target file! Output: TED2020.de-en.de-filtered.de.subword\n"
          ]
        }
      ],
      "source": [
        "sp.load(target_model)\n",
        "\n",
        "with open(target_filtered) as target, open(target_subworded, \"w+\") as target_subword:\n",
        "    for line in target:\n",
        "        line = line.strip()\n",
        "        line = sp.encode_as_pieces(line)\n",
        "        # line = ['<s>'] + line + ['</s>']    # add start & end tokens; unrequired for OpenNMT\n",
        "        line = \" \".join([token for token in line])\n",
        "        target_subword.write(line + \"\\n\")\n",
        "\n",
        "print(\"Done subwording the target file! Output:\", target_subworded)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an output, you should now have two subworded datasets:\n",
        "\n",
        "*   source dataset: `TED2020.de-en.en-filtered.en.subword`\n",
        "*   target dataset: `TED2020.de-en.de-filtered.de.subword`\n",
        "\n",
        "Note: We will cover **desubwording** at the end of the [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "j6x76VsQRSfA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohvfh5rzNiox"
      },
      "source": [
        "**Checking the Dataset** \n",
        "\n",
        "As before, you can check your dataset to make sure that they have been properly subworded. Run the cell below to print the first and last two lines of each filtered subworded file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "On1b6YPguzjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d926e505-0673-4884-d21e-2dded414959d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 Lines:\n",
            "▁He ▁said ▁that ▁he ▁had ▁been ▁in ▁the ▁Gulf ▁a ▁little ▁while ▁ago ; ▁like ▁about ▁a ▁week ▁ago , ▁and ▁a ▁guy ▁who ▁had ▁been ▁a ▁recreation al ▁fishing ▁guide ▁took ▁him ▁out ▁to ▁show ▁him ▁what ' s ▁going ▁on .\n",
            "▁On ▁the ▁other ▁side ▁of ▁the ▁country , ▁I ▁went ▁to ▁class ▁and ▁wrote ▁a ▁poem ▁in ▁the ▁margins ▁of ▁my ▁notebook .\n",
            "----\n",
            "▁Er ▁erzaehlt e ▁mir , ▁dass ▁er ▁vor ▁kurzem ▁am ▁Golf ▁gewesen ▁sei , ▁-- ▁vor ▁etwa ▁einer ▁Woche ▁-- ▁und ▁ein ▁Mann , ▁der ▁als ▁Tour guide ▁ fuer ▁Freizeit ang ler ▁gearbeitet ▁hatte , ▁nahm ▁ihn ▁mit , ▁um ▁ihm ▁zu ▁zeigen , ▁was ▁los ▁war .\n",
            "▁Auf ▁der ▁anderen ▁Seite ▁des ▁Landes ▁ging ▁ich ▁zur ▁Schule ▁und ▁schrieb ▁ein ▁Gedicht ▁an ▁meinen ▁Heft rand .\n",
            "\n",
            "Last 2 Lines:\n",
            "▁It ' s ▁just ▁love . ▁That ' s ▁all ▁it ▁is .\n",
            "▁Now , ▁why ▁are ▁algorithms ▁even ▁important ?\n",
            "-----\n",
            "▁Es ▁ist ▁Liebe . ▁Das ▁ist ▁alles .\n",
            "▁Warum ▁sind ▁Algorithmen ▁überhaupt ▁wichtig ?\n"
          ]
        }
      ],
      "source": [
        "print(\"First 2 Lines:\")\n",
        "!head -n 2 'TED2020.de-en.en-filtered.en.subword' && echo \"----\" && head -n 2 'TED2020.de-en.de-filtered.de.subword'\n",
        "print()\n",
        "print(\"Last 2 Lines:\")\n",
        "!tail -n 2 'TED2020.de-en.en-filtered.en.subword' && echo \"-----\" && tail -n 2 'TED2020.de-en.de-filtered.de.subword'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The underscores indicate that there is a space before the respective tokens, meaning that, in a sentence, word tokens with an underscore preceding them can act as independent words. Word tokens without an underscore (not included in our example sentences above) could only act as suffixes to other word tokens. If you can see a question mark token at the beginning of a sentence, for example, you'll notice it is missing an underscore. This means that there is no space before this token and that it therefore always follows a preceding token without a space between the two (which is how we actually use a question mark in language). This becomes clearer when you compare a subworded and a desubworded translation output in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "zK134hpbbNCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Datasets"
      ],
      "metadata": {
        "id": "LWrJsHYbRV2r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaZ5B156QkgS"
      },
      "source": [
        "Now that we have cleaned, filtered and subworded the source and target files in our dataset, we need to **split** this dataset for MT training purposes. \n",
        "\n",
        "To train an NMT model, three distinct sub-datasets (of both the source and the target files) are required:\n",
        "\n",
        "1. **training dataset** – used to actually train the model,\n",
        "2. **development dataset** – used to run regular validations during the training stage of the NMT model to help improve the model parameters,\n",
        "3. **test dataset** – used after the model is fully trained to evaluate the model on data the model has not yet seen in the training stage (to provide a test translation).\n",
        "\n",
        "The training dataset should be by far the largest of the three datasets and represent about 70-80% of the original dataset. Both the development and the test datasets can be of equal size of around 10-15% of the original dataset.\n",
        "\n",
        "Note: Our total TED2020 dataset contains 300,000 segments (as described in the introduction), so for this MT project, we define the development and test sub-datasets to consist of 35,000 segments each (a little more than 10% each, as indicated in the cell below). This means that the training dataset will consist of around 216,000 segments (almost 80%, as indicated above). Run the cell below to simultaneously split both the source and the target files of the original dataset into three sub-datasets: a training, development and a test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgNypY5CDQXT"
      },
      "source": [
        "1. Run the cell below to define the number of segments in the development (dev) and the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Jf7eoXQjAVNR"
      },
      "outputs": [],
      "source": [
        "segment_no_dev = 35000\n",
        "segment_no_test = 35000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwonTorXDYrt"
      },
      "source": [
        "2. Run the cell below to define a dataframe, using the subworded source `TED2020.de-en.en-filtered.en.subword` and target `TED2020.de-en.de-filtered.de.subword` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LjTFzLSIAW2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c184f3-6aeb-448e-efa9-2895ebcc220f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (286843, 2)\n"
          ]
        }
      ],
      "source": [
        "df_source = pd.read_csv(source_subworded, names=['Source'], sep=\"\\n\", quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
        "df_target = pd.read_csv(target_subworded, names=['Target'], sep=\"\\n\", quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
        "df = pd.concat([df_source, df_target], axis=1)  # Join the two dataframes along columns\n",
        "print(\"Dataframe shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Run the cell below to delete rows with empty cells (both in the source and target)."
      ],
      "metadata": {
        "id": "sFazwf7sRzpp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Kg7RPEG1AcJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1c0f06-44f3-4a50-a73e-a00006a0d285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty Cells Deleted --> Rows: 286843\n"
          ]
        }
      ],
      "source": [
        "df = df.dropna()\n",
        "\n",
        "print(\"Empty Cells Deleted\", \"--> Rows:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QejnQgKsDi62"
      },
      "source": [
        "4. Run the cell below to extract the development (dev) and test set from the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9DyhlM8PAw4-"
      },
      "outputs": [],
      "source": [
        "# Extract Dev set\n",
        "df_dev = df.sample(n = int(segment_no_dev))\n",
        "df_train = df.drop(df_dev.index)\n",
        "\n",
        "# Extract Test set\n",
        "df_test = df_train.sample(n = int(segment_no_test))\n",
        "df_train = df_train.drop(df_test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_iFWjn6DqZX"
      },
      "source": [
        "5. Run the cell below to write the dataframe to two source and target files.\n",
        "\n",
        "Note: The ending such as `-filtered.en.subword.train` can also be changed to just `.train` in the future to have shorter file names. We do **not** recommend to change the names here, as we refer to these data files *exactly* as they are named here in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing). So if you wish to train an NMT model from scratch using our tutorial, do not change the names of the files until you are fully confident to do so everywhere where necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "o4f8JPFNDM9I"
      },
      "outputs": [],
      "source": [
        "source_file_train = source_file+'-filtered.en.subword.train'\n",
        "target_file_train = target_file+'-filtered.de.subword.train'\n",
        "\n",
        "source_file_dev = source_file+'-filtered.en.subword.dev'\n",
        "target_file_dev = target_file+'-filtered.de.subword.dev'\n",
        "\n",
        "source_file_test = source_file+'-filtered.en.subword.test'\n",
        "target_file_test = target_file+'-filtered.de.subword.test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EZ2b8-1Duo9"
      },
      "source": [
        "6. Run the cell below to define the training dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tnIpXw-CA-WR"
      },
      "outputs": [],
      "source": [
        "df_dic_train = df_train.to_dict(orient='list')\n",
        "\n",
        "with open(source_file_train, \"w\") as sf:\n",
        "  sf.write(\"\\n\".join(line for line in df_dic_train['Source']))\n",
        "  sf.write(\"\\n\") # end of file newline\n",
        "\n",
        "with open(target_file_train, \"w\") as tf:\n",
        "  tf.write(\"\\n\".join(line for line in df_dic_train['Target']))\n",
        "  tf.write(\"\\n\") # end of file newline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMh9D6tnD0x3"
      },
      "source": [
        "7. Run the cell below to define the development dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8T5SlgOcBEdH"
      },
      "outputs": [],
      "source": [
        "df_dic_dev = df_dev.to_dict(orient='list')\n",
        "\n",
        "with open(source_file_dev, \"w\") as sf:\n",
        "  sf.write(\"\\n\".join(line for line in df_dic_dev['Source']))\n",
        "  sf.write(\"\\n\") # end of file newline\n",
        "        \n",
        "with open(target_file_dev, \"w\") as tf:\n",
        "  tf.write(\"\\n\".join(line for line in df_dic_dev['Target']))\n",
        "  tf.write(\"\\n\") # end of file newline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6_gIrK9D2rJ"
      },
      "source": [
        "8. Run the cell below to define the test dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "l8_tubkwBKuT"
      },
      "outputs": [],
      "source": [
        "df_dic_test = df_test.to_dict(orient='list')\n",
        "\n",
        "with open(source_file_test, \"w\") as sf:\n",
        "  sf.write(\"\\n\".join(line for line in df_dic_test['Source']))\n",
        "  sf.write(\"\\n\") # end of file newline\n",
        "        \n",
        "with open(target_file_test, \"w\") as tf:\n",
        "  tf.write(\"\\n\".join(line for line in df_dic_test['Target']))\n",
        "  tf.write(\"\\n\") # end of file newline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUjO4zc1D4YA"
      },
      "source": [
        "9. Run the cell below to show all the six files created: source files (training, development, test) and target files (training, development, test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "J9K0_7eI_rra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deef58c-57f8-44a2-9fb6-9eac77d52540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output files\n",
            "TED2020.de-en.en-filtered.en.subword.train\n",
            "TED2020.de-en.de-filtered.de.subword.train\n",
            "TED2020.de-en.en-filtered.en.subword.dev\n",
            "TED2020.de-en.de-filtered.de.subword.dev\n",
            "TED2020.de-en.en-filtered.en.subword.test\n",
            "TED2020.de-en.de-filtered.de.subword.test\n"
          ]
        }
      ],
      "source": [
        "print(\"Output files\", *[source_file_train, target_file_train, source_file_dev, target_file_dev, source_file_test, target_file_test], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0WYkHCIT5Rj"
      },
      "source": [
        "**Checking the Datasets** \n",
        "\n",
        "As before, you can check your datasets to make sure that they have been split properly. As mentioned above, you should have a total of six sub-datasets. Run the cell below to print the number of segments of each sub-dataset to confirm that they are of equal length, for both the source and the target dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5wbu0BF3wdv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7d4cf5-f83f-444e-f9a7-012f1e7c3100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line count for Source Train, Development and Test Datasets:\n",
            "216843 TED2020.de-en.en-filtered.en.subword.train\n",
            "35000 TED2020.de-en.en-filtered.en.subword.dev\n",
            "35000 TED2020.de-en.en-filtered.en.subword.test\n",
            "\n",
            "Line count for Target Train, Development and Test Datasets:\n",
            "216843 TED2020.de-en.de-filtered.de.subword.train\n",
            "35000 TED2020.de-en.de-filtered.de.subword.dev\n",
            "35000 TED2020.de-en.de-filtered.de.subword.test\n"
          ]
        }
      ],
      "source": [
        "print(\"Line count for Source Train, Development and Test Datasets:\")\n",
        "!wc -l 'TED2020.de-en.en-filtered.en.subword.train'\n",
        "!wc -l 'TED2020.de-en.en-filtered.en.subword.dev'\n",
        "!wc -l 'TED2020.de-en.en-filtered.en.subword.test'\n",
        "print()\n",
        "print(\"Line count for Target Train, Development and Test Datasets:\")\n",
        "!wc -l 'TED2020.de-en.de-filtered.de.subword.train'\n",
        "!wc -l 'TED2020.de-en.de-filtered.de.subword.dev'\n",
        "!wc -l 'TED2020.de-en.de-filtered.de.subword.test'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now selected, downloaded, cleaned, filtered, tokenized and split source and target datasets for MT training purposes!\n",
        "\n",
        "We could now feed these prepared datasets into an MT system to create a trained MT model that could be employed in an MT-assisted translation scenario. How to use this data to train an NMT model is shown in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "5lGSV9LBSFgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Datasets"
      ],
      "metadata": {
        "id": "hXJHbTQ8SGE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, our datasets are simply saved locally in this notebook. Of course we do not want to lose these datasets (which would happen if you closed this window or the runtime was interrupted). There are two ways to save the datasets. One option is to locally download and save them, and the second option is to connect this notebook to your Google Drive and directly save the datasets in a Google Drive folder. The second step is recommended as we will later access the prepared datasets in Google Drive to train an NMT model in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "6ADALBeTSIXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Saving Datasets Locally"
      ],
      "metadata": {
        "id": "ojxuKPCSSKSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the left-hand-side of this notebook, there is an icon indicating a folder with your files (as created in this session). By clicking on the three vertical dots on the right of the file you wish to download (for example,`TED2020.de-en.en-filtered.en.subword.train`), you can choose to download the file locally. This is also shown in the tutorial video. You can then save and upload the file on your notebook/desktop PC, or onto Google Drive. You can do this with all six prepared datasets (source train, dev and test dataset, and target train, dev and test dataset) and the subworded source and target models."
      ],
      "metadata": {
        "id": "Q-bljyTMSKya"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVsqC1JGoOeO"
      },
      "source": [
        "## Option 2: Connecting to Your Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another, preferred, option is to connect your notebook to your Google Drive and directly save the datasets into a Drive folder of your choice. To connect this notebook to Google Drive, run the cell below. You will need to select and confirm your Google Account. This step is also shown in the tutorial video.\n",
        "\n",
        "If connecting to Google Drive was successful, the cell below will output \"Drive Mounted\" or a similar message."
      ],
      "metadata": {
        "id": "nRJwps_FSPil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3WYuWXVnDWX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Copy your data to your Google Drive**\n",
        "\n",
        "Now that this Notebook is connected to your Google Drive, you can simply run the cell below to copy and save your six sub-datasets and your two subworded models to your Google Drive. \n",
        "\n",
        "**Do not** change the names of the files (datasets) as they need to match the variable names as we have been using them in this notebook for the NMT Training notebook.\n",
        "\n",
        "The path **'drive/MyDrive/'** is a must and the datasets would be directly saved into your main drive folder. Run the cell below to save the files into your general Drive folder."
      ],
      "metadata": {
        "id": "unZMq3xLcaLF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEhD19r8nPUB"
      },
      "outputs": [],
      "source": [
        "# Copy subworded models to your drive\n",
        "! cp source.model 'drive/MyDrive/'\n",
        "! cp target.model 'drive/MyDrive/'\n",
        "\n",
        "# Copy source files your drive\n",
        "! cp TED2020.de-en.en-filtered.en.subword.train 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.dev 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.test 'drive/MyDrive/'\n",
        "\n",
        "# Copy target files into your drive\n",
        "! cp TED2020.de-en.de-filtered.de.subword.train 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.dev 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.test 'drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've run the cell above, you can check your Google Drive and should find the six sub-datasets and the two subword models saved there. If you like, you can then change the names of the datasets directly in your drive for further use. You can also check whether you have successfully saved the datasets in your Google Drive by checking your Drive content. Run the following cell and the output should list your files."
      ],
      "metadata": {
        "id": "R_hOHEnnSZXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls 'drive/MyDrive/'"
      ],
      "metadata": {
        "id": "X_mWyTtMSn4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otherwise** you can preferrably **create a folder** for this purpose (let's say *MT_data_preparation*) and save the files directly into that folder. You would then need to change the path to your newly created folder *'drive/MyDrive/MT_data_preparation'*. This step is also shown in the tutorial video."
      ],
      "metadata": {
        "id": "1xJfRS_aSUv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Preferrably create your own folder MT_data_preparation and copy each file into that folder\n",
        "# Copy subworded models to your drive\n",
        "! cp source.model 'drive/MyDrive/MT_data_preparation/' \n",
        "! cp target.model 'drive/MyDrive/MT_data_preparation/'\n",
        "\n",
        "# Copy source files to your drive\n",
        "! cp TED2020.de-en.en-filtered.en.subword.train 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.dev 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.test 'drive/MyDrive/MT_data_preparation/'\n",
        "\n",
        "# Copy target files to your drive\n",
        "! cp TED2020.de-en.de-filtered.de.subword.train 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.dev 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.test 'drive/MyDrive/MT_data_preparation/'"
      ],
      "metadata": {
        "id": "0-VGsOw_yjt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have created your own folder *MT_data_preparation* and saved the files to that folder, run the cell below,"
      ],
      "metadata": {
        "id": "2GX6tvXRyyeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively run:\n",
        "!ls 'drive/MyDrive/MT_data_preparation'\n",
        "\n",
        "# Or alternatively\n",
        "#!ls 'drive/MyDrive/YOUR_FOLDER'"
      ],
      "metadata": {
        "id": "Ne4dVb23yxJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now successfully saved all MT datasets and subworded models created in this notebook. You will profit from these preparatory steps when training an actual NMT model, for example using our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "PZAV8u0vy1Pa"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}