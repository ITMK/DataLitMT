{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITMK/DataLitMT/blob/main/colab_notebook_basic_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![KeyVisual_DataLitMT - Kopie.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "# Data Planning and Data Collection – Basic Level\n",
        "**How to select, download, prepare and process data for training NMT models**"
      ],
      "metadata": {
        "id": "w8vG8tlOOGcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n"
      ],
      "metadata": {
        "id": "fHYJEeo5N7Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab Notebook is concerned with the Data Planning and Data Collection (& Production) dimensions of the [DataLit<sup>MT</sup> Competence Matrix](https://itmk.github.io/The-DataLitMT-Project/matrix/). The notebook guides you step-by-step through gathering, preparing and processing data in order to train a neural machine translation (NMT) model, using, for example the [OpenNMT-py toolkit](https://opennmt.net/OpenNMT-py/). These steps are the central preparatory steps of an MT training pipeline, which is covered under *Technical MT Literacy* in the [Professional Machine Translation Literacy Framework](https://itmk.github.io/The-DataLitMT-Project/framework/#professional-mt-literacy). The code sections of this notebook are based on [Yasmin Moslem's repository for MT data preparation](https://github.com/ymoslem/MT-Preparation). You might also be interested in [this TAUS article on data cleaning for natural language processing](https://www.taus.net/resources/blog/ten-step-guide-to-data-cleaning), which highlights the various steps of a data cleaning pipeline. The first five steps discussed in this article are also covered in this notebook.\n",
        "\n",
        "**General Information**\n",
        "\n",
        "NMT models are trained on large datasets in the form of bilingual text corpora, which are aligned at sentence-level. NMT models are very sensitive to defects (also called 'noise') in their training data, as described, for example, in [Khayrallah/Koehn (2018)](https://aclanthology.org/W18-2709/). Therefore, thoroughly preparing the training datasets (cleaning, filtering, aligning, tokenizing, etc.) is paramount for creating high-quality NMT models.\n",
        "\n",
        "**Note:** To avoid losing your data produced throughout this notebook, do **not** close this window until you have completed all steps and downloaded the datasets locally or saved them to your Google Drive! This step is illustrated at the end of this notebook.\n",
        "\n",
        "A walk-through of this notebook and more detailed information on gathering and preparing data is provided in the accompanying tutorial video.\n",
        "\n",
        "---\n",
        "**Steps to take**\n",
        "\n",
        "As outlined in our DataLit<sup>MT</sup> Competence Matrix, a number of steps have to be taken to acquire data for training an NMT model. A **data requirement analysis** will show you which bilingual (also called parallel) data will be needed for training an NMT model that suits your individual requirements (domain, language combination, etc.). For more information on how to create custom machine translation models, see the chapter by [Ramírez-Sánchez (2022)](https://zenodo.org/record/6760022/files/342-Kenny-2022-9.pdf). Based on your data requirement analysis, you can then develop a **data strategy**, which will outline the steps required to obtain the data. Taking into account relevant aspects of **data curation and protection**, you can then identify and evaluate suitable sources for downloading bilingual datasets. For example, one source of publicly available parallel datasets, which can be used for training NMT models, is the [OPUS corpus collection](https://opus.nlpl.eu). Most datasets in the OPUS collection are available for non-commercial (research) use and can be searched by simply selecting a source and a target language. Once you have found a suitable dataset, you can **verify** this data by checking the type of dataset (web-crawled etc.) and clicking on *sample* to check the quality of the data. For MT, [moses](http://www2.statmt.org/moses/index.php?n=Main.HomePage) is used as the preferred download format, as the sentences are already aligned (meaning that every line in the source file refers to that same line in the target file, i.e., these lines are translations of each other). You can then **acquire** the data by clicking on the *moses* field or by running the cell below (when you right-click on the *moses* field of the dataset you selected on the OPUS website, you can copy the link to download the dataset. This is also shown in the accompanying tutorial video).\n",
        "\n",
        "For our current MT project scenario, we choose to work with the English-German parallel dataset *TED2020*, where we chose English as the source and German as the target language. When running the cell below, a .zip file will be downloaded and the two files we're interested in are those that end on the language codes: `.en` and `.de`. The data we prepare and save in this notebook can then be used to train an NMT model from scratch using our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing).\n",
        "\n",
        "Note: TED2020 is a comparatively small dataset for NMT training, containing 'only' 300,000 parallel sentences (more information on this dataset can be found [here](https://opus.nlpl.eu/TED2020.php)). This is sufficient to train a demo NMT model and to go through all steps required. Note, however, that datasets for training more efficient and high-performing NMT models, usually contain millions of sentences.\n",
        "\n",
        "Run the following cell to download the dataset from the OPUS collection."
      ],
      "metadata": {
        "id": "YDH-ZRRrTjUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/de-en.txt.zip\n",
        "!unzip de-en.txt.zip"
      ],
      "metadata": {
        "id": "2mDd6NL7mHlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a1137e-e174-497a-da84-d44c889c31cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-17 17:12:17--  https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/de-en.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21857900 (21M) [application/zip]\n",
            "Saving to: ‘de-en.txt.zip’\n",
            "\n",
            "de-en.txt.zip       100%[===================>]  20.84M  12.5MB/s    in 1.7s    \n",
            "\n",
            "2023-02-17 17:12:20 (12.5 MB/s) - ‘de-en.txt.zip’ saved [21857900/21857900]\n",
            "\n",
            "Archive:  de-en.txt.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: TED2020.de-en.de        \n",
            "  inflating: TED2020.de-en.en        \n",
            "  inflating: TED2020.de-en.xml       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should give you the two monolingual files:\n",
        "\n",
        "*   `TED2020.de-en.en`\n",
        "*   `TED2020.de-en.de`\n",
        "\n",
        "Note: We will not be working with the third file, i.e., the xml file.\n",
        "\n",
        "Each file has a sentence/segment per line that is a matching (aligned) translation of the same line in the other file (this is called [parallel text format](https://google.github.io/seq2seq/nmt/)).\n",
        "\n",
        "Note: If you wish to download a different dataset, just change the URL in the code cell above (make sure, that the data is still in *moses* format)."
      ],
      "metadata": {
        "id": "vwgfmA3u_K-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "631NzlK5N394"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we move on to filtering, **preparing and processing** this dataset. At the end of the notebook, we will show you how to download the resulting datasets to a local drive or how to save them to your Google Drive. After making sure that the data intended to train an NMT model is a bilingual dataset aligned at sentence-level, we can take different steps in order to clean this MT training data. These steps are outlined below."
      ],
      "metadata": {
        "id": "MKxPsiwFqpBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Necessary Library-Requirements**\n",
        "\n",
        "First, we'll install a range of necessary python libraries which are required for our data preparation steps. To do so, simply run the cells below."
      ],
      "metadata": {
        "id": "vKn8NgtAjb6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sacremoses\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "Ur-AigWEla2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning the DataLit<sup>MT</sup>-GitHub Repository**\n",
        "\n",
        "Next, we'll clone our [DataLit<sup>MT</sup> Github Repository](https://github.com/ITMK/DataLitMT) in order to get access to pre-written code that we'll use in this notebook. Again, just run the cell below."
      ],
      "metadata": {
        "id": "tL3V4hRJl2Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ITMK/DataLitMT.git"
      ],
      "metadata": {
        "id": "4ZEmFdBjg-f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Datasets and Languages**\n",
        "\n",
        "Now, we'll define our source file and language (source dataset, English) as well as our target file and language (target dataset, German) based on the *TED2020* dataset we downloaded in the previous step. Simply run the cell below. From now on, we'll work with the variables `source_file`, `target_file`, `source_lang` and `target_lang`."
      ],
      "metadata": {
        "id": "slRtFyF9BuZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_file = \"TED2020.de-en.en\"\n",
        "target_file = \"TED2020.de-en.de\"\n",
        "source_lang = \"en\"\n",
        "target_lang = \"de\""
      ],
      "metadata": {
        "id": "wyg9wXL7nGob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you have downloaded a different dataset, simply change the names and languages (marked in red) in the strings above to match your data."
      ],
      "metadata": {
        "id": "eL63plU4CTnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering & Cleaning Datasets for MT Training**\n",
        "\n",
        "Now we move on to actually filtering and cleaning the dataset, as required for MT training purposes. This step will take the source and target files of the dataset and delete rows that contain empty cells, delete duplicates, source-copied rows and source/target sentences that are too long, remove HTML code, list how many rows will remain in true-cased form, delete rows with empty cells, shuffle rows and finally save the source and target files (= the filtered dataset). You can find more information on these data preparation steps in section 3 of [Bui et al. (2020)](https://aclanthology.org/2020.eamt-1.35). The final files will be adequately filtered and cleaned for our NMT training purposes. Again, run the cell below."
      ],
      "metadata": {
        "id": "6b-2vCHxnGRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments: source file, target file, source language, target language\n",
        "!python3 DataLitMT/learning_resources/data_planning_and_collection/filter.py TED2020.de-en.en TED2020.de-en.de en de"
      ],
      "metadata": {
        "id": "ItYuDOp2hg5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an output, you should now have the two filtered and cleaned source and target files:\n",
        "\n",
        "*  source data: `TED2020.de-en.en-filtered.en`\n",
        "*  target data: `TED2020.de-en.de-filtered.de`\n",
        "\n"
      ],
      "metadata": {
        "id": "ev2kOZGGEcRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the Dataset** \n",
        "\n",
        "It is always good practice to check your dataset in between steps to make sure that everything has worked well. For example, you could check that sentences are properly aligned. Run the cell below to print the first and last two lines of the filtered and cleaned source and target files."
      ],
      "metadata": {
        "id": "fxaG10rO9K-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 2 Lines:\")\n",
        "!head -n 2 'TED2020.de-en.en-filtered.en' && echo \"----\" && head -n 2 'TED2020.de-en.de-filtered.de'\n",
        "print()\n",
        "print(\"Last 2 Lines:\")\n",
        "!tail -n 2 'TED2020.de-en.en-filtered.en' && echo \"-----\" && tail -n 2 'TED2020.de-en.de-filtered.de'"
      ],
      "metadata": {
        "id": "_X-WZGV89J2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Subword) Tokenizing with SentencePiece"
      ],
      "metadata": {
        "id": "jEWkbgBnP1zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next step of our data preparation pipeline, we will tokenize the previously filtered datasets. This is a necessary step for MT training. Unlike humans, a computer does not read text in the form of individual words and sentences, but rather as a continuous string of characters (blank spaces included). Therefore, text (vocabulary) used to train MT models (and other Natural Language Processing or NLP models) must be tokenized so that the models know where the boundaries between individual words (and sentences) are.\n",
        "\n",
        "A special form of tokenization is subwording, where words are split into smaller units (so-called subwords) from which complete words can be assembled. The subword tokenization method is described in more detail in [Sennrich et al. (2016)](https://aclanthology.org/P16-1162/). NMT models are usually trained with such subwords instead of whole words in order to reduce the size of the NMT model’s vocabulary (the larger the vocabulary, the higher the data processing requirements will be). For example, the four words *low*, *lowest*, *high* and *highest* can be represented by the three subwords *low*, *high* and *est*, which would reduce the required vocabulary size of the NMT model by 1. During translation, when the MT model comes across a new word/token that resembles a word/token that is also present in its training vocabulary (or can be assembled from the subwords in this vocabulary), the MT model will be able to translate this word/token. Otherwise, it would produce an “unk” (= “unknown”) token as output. If you would like to know more about (subword) tokenization for training NLP models, have a look at [this article](https://blog.floydhub.com/tokenization-nlp/).\n",
        "\n",
        "A preferred toolkit for subword tokenization is [SentencePiece](https://github.com/google/sentencepiece). To apply SentencePiece, a subwording model must first be trained on the source and target files of the dataset. This model can then be applied to these files in order to subword them. These subworded source and target files are then used in the MT training process. When the trained model is used for translating, it will produce a subworded translation. Using the same SentencePiece model, this subworded translation must then be “desubworded” or “decoded” back into fluent text. Note that in systems such as DeepL or Google Translate, these subwording and desubwording steps are performed \"backstage\" so that we as users are not aware of them.\n",
        "\n",
        "Run the code below to train a SentencePiece model for subword tokenization based on the two previously filtered files: `TED2020.de-en.en-filtered.en` and `TED2020.de-en.de-filtered.de`."
      ],
      "metadata": {
        "id": "2qnzrOHE2Stn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a SentencePiece model for subword tokenisation\n",
        "!python DataLitMT/learning_resources/data_planning_and_collection/train_unigram.py TED2020.de-en.en-filtered.en TED2020.de-en.de-filtered.de"
      ],
      "metadata": {
        "id": "rGSQ_bX5j209"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now trained two separate SentencePiece subwording models: `source.model` and `target.model`. At the end of this notebook, we will also save the subworded source and target model as these will be needed for future subwording and desubwording operations (the subwording and desubwording step is also required in the [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing))."
      ],
      "metadata": {
        "id": "kWoxN-8vI2Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subwording the filtered source and target datasets**\n",
        "\n",
        "By applying these source and target SentencePiece models, our previously filtered files can now be subworded. To do so, run the code below."
      ],
      "metadata": {
        "id": "TWvajFHNNQ51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword the dataset\n",
        "!python3 DataLitMT/learning_resources/data_planning_and_collection/subword.py source.model target.model TED2020.de-en.en-filtered.en TED2020.de-en.de-filtered.de"
      ],
      "metadata": {
        "id": "8CXIPV4okyrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an output, you should now have two subworded datasets:\n",
        "\n",
        "*   source dataset: `TED2020.de-en.en-filtered.en.subword`\n",
        "*   target dataset: `TED2020.de-en.de-filtered.de.subword`\n",
        "\n",
        "Note: We will cover **desubwording** at the end of the [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "QCwNWF76JOHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the Dataset** \n",
        "\n",
        "Again, you can check your dataset to make sure that they have been properly subworded. Run the cell below to print the first and last two lines of each filtered and subworded file."
      ],
      "metadata": {
        "id": "ohvfh5rzNiox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 2 Lines:\")\n",
        "!head -n 2 'TED2020.de-en.en-filtered.en.subword' && echo \"----\" && head -n 2 'TED2020.de-en.de-filtered.de.subword'\n",
        "print()\n",
        "print(\"Last 2 Lines:\")\n",
        "!tail -n 2 'TED2020.de-en.en-filtered.en.subword' && echo \"-----\" && tail -n 2 'TED2020.de-en.de-filtered.de.subword'"
      ],
      "metadata": {
        "id": "On1b6YPguzjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The underscores indicate that there is a space before the respective tokens, meaning that, in a sentence, word tokens with an underscore preceding them can act as independent words. Word tokens without an underscore (not included in our example sentences above) could only act as suffixes to other word tokens. If you can see a question mark token at the beginning of a sentence, for example, you'll notice it is missing an underscore. This means that there is no space before this token and that it therefore always follows a preceding token without a space between the two (which is how we actually use a question mark in language). This becomes clearer when you compare a subworded and a desubworded translation output in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "I1qZQJVrSeoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Datasets"
      ],
      "metadata": {
        "id": "jNkOCADsP55C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have cleaned, filtered and subworded the source and target files in our dataset, we need to **split** this dataset for MT training purposes. \n",
        "\n",
        "To train an NMT model, three distinct sub-datasets (of both the source and the target files) are required:\n",
        "\n",
        "1. **training dataset** – used to actually train the model,\n",
        "2. **development dataset** – used to run regular validations during the training stage of the NMT model to help improve the model parameters,\n",
        "3. **test dataset** – used after the model is fully trained to evaluate the model on data the model has not yet seen in the training stage (to provide a test translation).\n",
        "\n",
        "The training dataset should be by far the largest of the three datasets and represent about 70-80% of the original dataset. Both the development and the test datasets can be of equal size of around 10-15% of the original dataset.\n",
        "\n",
        "Note: Our total TED2020 dataset contains 300,000 segments (as described in the introduction), so for this MT project, we define the development and test sub-datasets to consist of 35,000 segments each (a little more than 10% each, as indicated in the cell below). This means that the training dataset will consist of 216,843 segments (almost 80%, as indicated above). Run the cell below to simultaneously split both the source and the target files of the original dataset into three sub-datasets: a training, development and a test dataset."
      ],
      "metadata": {
        "id": "DaZ5B156QkgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training set, development set, and test set\n",
        "!python DataLitMT/learning_resources/data_planning_and_collection/train_dev_test_split.py 35000 35000 TED2020.de-en.en-filtered.en.subword TED2020.de-en.de-filtered.de.subword"
      ],
      "metadata": {
        "id": "T5istErJlLj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now have the following six sub-datasets:\n",
        "\n",
        "*  source training dataset: `TED2020.de-en.en-filtered.en.subword.train`\n",
        "*  source development dataset: `TED2020.de-en.en-filtered.en.subword.dev`\n",
        "*  source test dataset: `TED2020.de-en.en-filtered.en.subword.test`\n",
        "*  target training dataset: `TED2020.de-en.de-filtered.de.subword.train`\n",
        "*  target development dataset: `TED2020.de-en.de-filtered.de.subword.dev`\n",
        "*  target test (reference) dataset: `TED2020.de-en.de-filtered.de.subword.test`\n"
      ],
      "metadata": {
        "id": "cMBBedE5syv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking the Datasets** \n",
        "\n",
        "As before, you can check your datasets to make sure that they have been split properly. As mentioned above, you should have a total of six sub-datasets. Run the cell below to print the number of segments of each sub-dataset to confirm that they are of equal length, for both the source and the target dataset."
      ],
      "metadata": {
        "id": "M0WYkHCIT5Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Line count for Source Train, Development and Test Datasets:\")\n",
        "!wc -l 'TED2020.de-en.en-filtered.en.subword.train'\n",
        "!wc -l 'TED2020.de-en.en-filtered.en.subword.dev'\n",
        "!wc -l 'TED2020.de-en.en-filtered.en.subword.test'\n",
        "print()\n",
        "print(\"Line count for Target Train, Development and Test Datasets:\")\n",
        "!wc -l 'TED2020.de-en.de-filtered.de.subword.train'\n",
        "!wc -l 'TED2020.de-en.de-filtered.de.subword.dev'\n",
        "!wc -l 'TED2020.de-en.de-filtered.de.subword.test'"
      ],
      "metadata": {
        "id": "5wbu0BF3wdv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305c08be-39e1-40f5-adca-644c7a9c52ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line count for Source Train, Development and Test Datasets:\n",
            "216843 TED2020.de-en.en-filtered.en.subword.train\n",
            "35000 TED2020.de-en.en-filtered.en.subword.dev\n",
            "35000 TED2020.de-en.en-filtered.en.subword.test\n",
            "\n",
            "Line count for Target Train, Development and Test Datasets:\n",
            "216843 TED2020.de-en.de-filtered.de.subword.train\n",
            "35000 TED2020.de-en.de-filtered.de.subword.dev\n",
            "35000 TED2020.de-en.de-filtered.de.subword.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now selected, downloaded, cleaned, filtered, tokenized and split source and target datasets for MT training purposes!\n",
        "\n",
        "We could now feed these prepared datasets into an MT system to create a trained MT model that could be employed in an MT-assisted translation scenario. How to use this data to train an NMT model is shown in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "Nu_ACeqZK8Cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Datasets"
      ],
      "metadata": {
        "id": "ZHrxAaOqm9YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, our datasets are simply saved locally in this notebook. Of course we do not want to lose these datasets (which would happen if you closed this window or the runtime was interrupted). There are two ways to save the datasets. One option is to locally download and save them, and the second option is to connect this notebook to your Google Drive and directly save the datasets in a Google Drive folder. The second step is recommended as we will later access the prepared datasets in Google Drive to train an NMT model in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "nSGYLr9iLlEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Saving Datasets Locally"
      ],
      "metadata": {
        "id": "pr5zlUwrAYDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the left-hand-side of this notebook, there is an icon indicating a folder with your files (as created in this session). By clicking on the three vertical dots on the right of the file you wish to download (for example,`TED2020.de-en.en-filtered.en.subword.train`), you can choose to download the file locally. This is also shown in the tutorial video. You can then save and upload the file on your notebook/desktop PC, or onto Google Drive. You can do this with all six prepared datasets (source train, dev and test dataset, and target train, dev and test dataset) and the subworded source and target models."
      ],
      "metadata": {
        "id": "R5iurSTuL611"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Connecting to Your Google Drive"
      ],
      "metadata": {
        "id": "q9J4rdFfAWFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another, preferred, option is to connect this notebook to your Google Drive and directly save the datasets into a Drive folder of your choice. To connect this notebook to Google Drive, run the cell below. You will need to select and confirm your Google Account. This step is also shown in the tutorial video.\n",
        "\n",
        "If connecting to Google Drive was successful, the cell below will output “Drive Mounted”  or a similar message."
      ],
      "metadata": {
        "id": "-gFZ8oWcMc7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V3WYuWXVnDWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd38827c-6938-4c21-d6d0-1d959b3a21ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Copy your data to your Google Drive**\n",
        "\n",
        "Now that this notebook is connected to your Google Drive, you can simply run the cell below to copy and save your six sub-datasets and your two subworded models to your Google Drive. \n",
        "\n",
        "**Do not** change the names of the files (datasets) as they need to match the variable names as we have been using them in this notebook for the NMT Training notebook.\n",
        "\n",
        "The path **'drive/MyDrive/'** is a must and the datasets would be directly saved into your main drive folder. Run the cell below to save the files into your general Drive folder."
      ],
      "metadata": {
        "id": "CVsqC1JGoOeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy subworded models to your drive\n",
        "! cp source.model 'drive/MyDrive/'\n",
        "! cp target.model 'drive/MyDrive/'\n",
        "\n",
        "# Copy source files to your drive\n",
        "! cp TED2020.de-en.en-filtered.en.subword.train 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.dev 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.test 'drive/MyDrive/'\n",
        "\n",
        "# Copy target files to your drive\n",
        "! cp TED2020.de-en.de-filtered.de.subword.train 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.dev 'drive/MyDrive/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.test 'drive/MyDrive/'"
      ],
      "metadata": {
        "id": "rEhD19r8nPUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've run the cell above, you can check your Google Drive and should find the six sub-datasets and the two subword models saved there. If you like, you can then change the names of the datasets directly in your drive for further use. You can also check whether you have successfully saved the datasets in your Google Drive by checking your Drive content. Run the following cell and the output should list your files."
      ],
      "metadata": {
        "id": "E_ITlOh2Ngza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls 'drive/MyDrive/'"
      ],
      "metadata": {
        "id": "od_tPpAYSygx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otherwise** you can **create a folder** for this purpose (let's say *MT_data_preparation*) and save the files directly into that folder (this is a preferred option). You would then need to change the path to your newly created folder *'drive/MyDrive/MT_data_preparation'*. This step is also shown in the tutorial video."
      ],
      "metadata": {
        "id": "GM7_Y5pRu1EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Preferably create your own folder MT_data_preparation and copy each file into that folder\n",
        "# Copy subworded models to your drive\n",
        "! cp source.model 'drive/MyDrive/MT_data_preparation/' \n",
        "! cp target.model 'drive/MyDrive/MT_data_preparation/'\n",
        "\n",
        "# Copy source files to your drive\n",
        "! cp TED2020.de-en.en-filtered.en.subword.train 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.dev 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.en-filtered.en.subword.test 'drive/MyDrive/MT_data_preparation/'\n",
        "\n",
        "# Copy target files to your drive\n",
        "! cp TED2020.de-en.de-filtered.de.subword.train 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.dev 'drive/MyDrive/MT_data_preparation/'\n",
        "! cp TED2020.de-en.de-filtered.de.subword.test 'drive/MyDrive/MT_data_preparation/'"
      ],
      "metadata": {
        "id": "TKLY4IAQuzgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have created your own folder *MT_data_preparation* and saved the files to that folder, run the cell below,"
      ],
      "metadata": {
        "id": "RZAyFZ2ivUZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively run:\n",
        "!ls 'drive/MyDrive/MT_data_preparation'"
      ],
      "metadata": {
        "id": "Iq9kY6NWvTey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b1b181-db80-4ef4-a0b0-64c8746e9d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source.model\n",
            "target.model\n",
            "TED2020.de-en.de-filtered.de.subword.dev\n",
            "TED2020.de-en.de-filtered.de.subword.test\n",
            "TED2020.de-en.de-filtered.de.subword.train\n",
            "TED2020.de-en.en-filtered.en.subword.dev\n",
            "TED2020.de-en.en-filtered.en.subword.test\n",
            "TED2020.de-en.en-filtered.en.subword.train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now successfully saved all MT datasets and subworded models created in this notebook. You will profit from these preparatory steps when training an actual NMT model, as we will show you in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "DvBBm9K5vcH4"
      }
    }
  ]
}
