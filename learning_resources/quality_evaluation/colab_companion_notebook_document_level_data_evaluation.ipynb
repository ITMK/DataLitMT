{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LntKRoYSHhe"
      },
      "source": [
        "![KeyVisual_DataLitMT - Kopie.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "# Data Evaluation – Companion Notebook for MT Quality Evaluation at Document Level\n",
        "**MT Quality Score Calculator at Document Level for Metrics Based on String Matching and for Embedding-based Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3FxGjAG7E-_"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is a companion notebook for our [Basic](https://colab.research.google.com/drive/1KY2qGmDoJOSOPewseVszL0u8J-x7xa7I?usp=sharing) and [Advanced](https://colab.research.google.com/drive/1UgsqgN-6yfDESU7Geei4RXzJShEQNViZ?usp=sharing) MT Quality Evaluation notebooks, which allows you to calculate MT quality scores for an entire text. We implement this document-level evaluation for the four metrics covered in our main notebooks, i.e., BLEU and TER, BERTScore and COMET. \n",
        "\n",
        "The code used for these calculations is adopted largely from the Natural Language Toolkit ([NLTK](https://www.nltk.org/)) and the respective GitHub repositories of each metric, as referenced in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giCwQH7e9sQg"
      },
      "source": [
        "## 0 Housekeeping\n",
        "\n",
        "You can run all cells in this notebook without having to change any of the code.\n",
        "However, in the examples discussed below, feel free to indicate your own (text) files, if you'd like to calculate individual scores. In our tutorial video, we will show you what you need to do if you want to deviate from the workflow illustrated in this video to calculate document-level scores for your own texts.\n",
        "\n",
        "We first need to ensure that we installed/upgraded pip. To do so, run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyqBfxnx6hqp"
      },
      "outputs": [],
      "source": [
        "# Upgrade to the current version of pip\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Your Own Documents\n",
        "\n",
        "Note: This step is only relevant, if you would also like to evaluate the quality of your own documents. Otherwise, continue to the next step 'Clone our GitHub Repository'.\n",
        "\n",
        "If you would like to know how to upload your own documents to calculate the metrics covered in this notebook, you can look at the accompanying tutorial video that will guide you through that process.\n",
        "\n",
        "To upload documents from your Google Drive, you would need to run the following cell and change the directory to **your folder**. If you would like, you can also access TED2020 documents, as we use in our [Data Planning and Data Collection](https://colab.research.google.com/drive/17aCfPF0Zw80gW0FYg_iRo16YVSMAaNKG?usp=sharing) and our [Data Processing](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing) resources, in [this GitHub folder](https://github.com/ITMK/DataLitMT/tree/main/learning_resources/quality_evaluation/TED). These steps are shown in the tutorial video."
      ],
      "metadata": {
        "id": "13DEDumNqxdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to your folder\n",
        "%cd /content/drive/MyDrive/YOUR_FOLDER/\n",
        "!ls"
      ],
      "metadata": {
        "id": "Qo5xq5H58K8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to assign variables (such as *source*, *reference* and *hypothesis*) to your files to check that they are of the same length. In the code cell below, change the names of your reference, hypothesis and source. Then run the cell."
      ],
      "metadata": {
        "id": "XMwBXE0i8ruS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('YOUR_REFERENCE', 'r') as reader:\n",
        "  reference = reader.readlines()\n",
        "\n",
        "with open('YOUR_HYPOTHESIS', 'r') as reader:\n",
        "  hypothesis = reader.readlines()\n",
        "\n",
        "with open('YOUR_SOURCE', 'r') as reader:\n",
        "  source = reader.readlines()\n",
        "\n",
        "print('Length of reference', len(reference))\n",
        "print('Length of hypothesis', len(hypothesis))\n",
        "print('Length of source', len(source))"
      ],
      "metadata": {
        "id": "q0OI3Gio8sfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate average scores for your files\n",
        "\n",
        "Run the cells below to calculate the BLEU, TER, BERTScore and COMET scores. Make sure, all packages are still installed, otherwise re-install them for each metric (in sections 1.0, 3.0 and 4.0)."
      ],
      "metadata": {
        "id": "wkjEpOPR6uil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate an average sacreBLEU score for your text file\n",
        "!sacrebleu YOUR_REFERENCE -i YOUR_HYPOTHESIS -m bleu --score-only --width 2"
      ],
      "metadata": {
        "id": "XR6K9CvG6iD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate an average TER for your text file\n",
        "!sacrebleu YOUR_REFERENCE -i YOUR_HYPOTHESIS -m ter --score-only --width 2"
      ],
      "metadata": {
        "id": "Sh9DfAnZ6f2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate an average BERTScore for your text file\n",
        "!bert-score -r YOUR_REFERENCE -c YOUR_HYPOTHESIS --lang de"
      ],
      "metadata": {
        "id": "J5NbSosX6jCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating an average COMETinho Score for your text file\n",
        "!comet-score -s YOUR_SOURCE -t YOUR_HYPOTHESIS -r YOUR_REFERENCE --model eamt22-cometinho-da --gpu 0"
      ],
      "metadata": {
        "id": "Mbxck7OX6i7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating an average Quality Estimation Score for your text file\n",
        "!comet-score -s YOUR_SOURCE -t YOUR_HYPOTHESIS --model wmt21-comet-qe-mqm --gpu 0"
      ],
      "metadata": {
        "id": "X2Xn8UYy6qwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nyqNHa8QOR8"
      },
      "source": [
        "## Clone our GitHub Repository\n",
        "\n",
        "First, we need to clone our [DataLit<sup>MT</sup> GitHub Repository](https://github.com/ITMK/DataLitMT/tree/main) to access the three text files we'll be working with in this notebook. Simply run the  code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6khzt48QSw0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ITMK/DataLitMT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPX-gEs-ZfQX"
      },
      "source": [
        "Let's now change the directory directly to the folder that contains the three files so we can easily refer to the files throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P-R9r3c5ZWoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedc6b05-8935-41e2-fbde-c96f6203c3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DataLitMT/learning_resources/quality_evaluation/example_texts\n",
            "HT_reference  NMT_hypothesis  source\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/DataLitMT/learning_resources/quality_evaluation/example_texts/\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files we use in this notebook are taken from our [Data Evaluation notebook on machine translationese and post-editese](https://colab.research.google.com/drive/1H_Nn-TRbOJlGPp2tmcJ_EAdIhlDC-xVK?usp=sharing). Ideally, have a look at these files to see what you will be working with. On the left-hand-side of this notebook, there is an icon indicating a folder with your files. By clicking on the three vertical dots on the right of the files of `NMT_hypothesis`, `HT_reference` and `source`, you can download these and open them locally on your desktop. These steps will also be shown in our tutorial video.\n",
        "\n",
        "To ensure the files are of the same length (the documents need to have an exact same number of sentences), you can run the code cell below. Each file should have a length of 16 (16 sentences). Having the same length is required to calculate automatic quality metrics at document level, so checking the length of the files is good practice."
      ],
      "metadata": {
        "id": "zeGKjWxWM0vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('HT_reference', 'r') as reader:\n",
        "  reference = reader.readlines()\n",
        "\n",
        "with open('NMT_hypothesis', 'r') as reader:\n",
        "  hypothesis = reader.readlines()\n",
        "\n",
        "with open('source', 'r') as reader:\n",
        "  source = reader.readlines()\n",
        "\n",
        "print('Length of reference', len(reference))\n",
        "print('Length of hypothesis', len(hypothesis))\n",
        "print('Length of source', len(source))"
      ],
      "metadata": {
        "id": "sN11RQg_EFxX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfa8f1b-ad44-4ffa-c1e1-98200a74e782"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of reference 16\n",
            "Length of hypothesis 16\n",
            "Length of source 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHoqS9kl-3G-"
      },
      "source": [
        "# 1 BLEU (sacreBLEU implementation)\n",
        "\n",
        "Here, we calculate document-level BLEU scores using the [sacreBLEU implementation](https://github.com/mjpost/sacrebleu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF9AIabIM23R"
      },
      "source": [
        "## 1.0 Importing Necessary Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZXrmuZQCT59"
      },
      "outputs": [],
      "source": [
        "# Import sacreBLEU functions\n",
        "!pip install sacrebleu\n",
        "import sacrebleu\n",
        "from sacrebleu.metrics import BLEU, TER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwbGKCzX_K5u"
      },
      "source": [
        "## 1.1 Calculating an average sacreBLEU score for an entire text\n",
        "\n",
        "We have the machine-translated text file `NMT_hypothesis` and the human-translated reference text `HT_reference`. Simply run the code cell below to calculate an average sacreBLEU score for the entire text.\n",
        "\n",
        "Note: In the code cell below, `bleu` indicates that we want to calculate the BLEU score. The sacreBLEU package also allows calculating other metrics such as TER (as shown below). `--score-only` (could also be written as `-b`) indicates that we only want the actual score to be printed (i.e., we want no detailed information on score calculation). If you are interested in the details, feel free to delete `--score-only` in the cell below. Similarly, `--width 2` (could also be written as `--w`) indicates that we only want to have two values after the decimal point. This number could be changed or `--width 2` could be removed to show only one value after the decimal point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate an average sacreBLEU score for the entire text file\n",
        "!sacrebleu HT_reference -i NMT_hypothesis -m bleu --score-only --width 2"
      ],
      "metadata": {
        "id": "83u3bE4-D8bF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbb69fa-2cb2-41c1-d718-0a650f2dce5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.69\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything went well, we should get an average sacreBLEU score for the *NMT_hypothesis* text of **34.69**. For information on how to interpret this score and the other scores calculated in this notebook, see our main notebooks on MT quality evaluation."
      ],
      "metadata": {
        "id": "fTNaOBsTU6rw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba_0iuqJ1V24"
      },
      "source": [
        "##2 TER\n",
        "\n",
        "Here, we calculate document-level TER. To do so, we can also use the [sacreBLEU package](https://github.com/mjpost/sacreBLEU#ter) installed above. In our BLEU calculation above, we indicated `bleu` in the code cell. Using the same sacreBLEU package, we can simply indicate `ter` in the code cell below to calculate an average TER score for the entire text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSm1BgW91kj4"
      },
      "source": [
        "## 2.1 Calculating an average TER score for an entire text\n",
        "\n",
        "We have the machine-translated text file `NMT_hypothesis` and the human-translated reference text `HT_reference`. Simply run the code cell below to calculate an average TER score for the entire text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TER\n",
        "!sacrebleu HT_reference -i NMT_hypothesis -m ter --score-only --width 2"
      ],
      "metadata": {
        "id": "3Rseao2bJPX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bd2c57-4c6f-41a7-c37b-cd607d25a160"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56.80\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, if everything went well, we should get an average TER score for the *NMT_hypothesis* text of **56.80**."
      ],
      "metadata": {
        "id": "BJdy50twVBMc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DNPdx14nQ28"
      },
      "source": [
        "# 3 BERTScore\n",
        "\n",
        "Here, we calculate document-level BERTScore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFXUUb3Rwh-U"
      },
      "source": [
        "## 3.0 Installing BERTScore\n",
        "\n",
        "First, we need to install the packages required to compute BERTScore. BERTScore is implemented as part of the bert-score package available in the [official BERTScore GitHub repository](https://github.com/Tiiiger/bert_score). Since embedding-based metrics rely on large embedding models, installing these metrics will usually take considerably longer than installing and importing traditional MT quality scores."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "1XhsQwWMSSXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PsYzOE7zUhR"
      },
      "source": [
        "## 3.1 Calculating an average BERTScore for an entire text\n",
        "\n",
        "We have the machine-translated text file `NMT_hypothesis` and the human-translated reference text `HT_reference`. Simply run the code cell below to calculate an average BERTScore score for the entire text. This may take a little while.\n",
        "\n",
        "Note: In this notebook we use `--lang de`, as the output texts we are working with are German. If you are using texts with a different language, you need to change the language code in the cell below. This step is also shown in our tutorial video."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bert-score -r HT_reference -c NMT_hypothesis --lang de"
      ],
      "metadata": {
        "id": "7IQQhrpYGgpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5351d5d-3a9b-4763-8d28-eec827c5c50d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-27 11:44:29.544323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-27 11:44:31.757741: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 11:44:31.757919: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 11:44:31.757952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Downloading (…)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 3.53kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 625/625 [00:00<00:00, 83.4kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 996k/996k [00:00<00:00, 3.22MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 4.55MB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 714M/714M [00:04<00:00, 177MB/s]\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.26.1)_fast-tokenizer P: 0.874346 R: 0.869231 F1: 0.871666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following three BERTScores should have been calculated:\n",
        "\n",
        "*   Precision `P: 0.874346`\n",
        "*   Recall `R: 0.869231`\n",
        "*   F-Measure `F1: 0.871666`\n",
        "\n",
        "The average BERTScore (F1 score) calculated for the entire *NMT_hypothesis* text is therefore **0.871666**."
      ],
      "metadata": {
        "id": "vyl7N-cfSrJU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfNrPeYnBdj"
      },
      "source": [
        "# 4 COMET\n",
        "\n",
        "Here, we calculate document-level COMET scores."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 Installing COMETinho\n",
        "First, we need to install the packages required to compute COMETinho. COMETinho is implemented as part of the unbabel-comet package available in the [official COMET GitHub repository](https://github.com/Unbabel/COMET).\n",
        "\n",
        "Note: Running this cell might result in requesting you to restart the runtime of this notebook: \"You must restart the runtime in order to use the newly installed version\". You can restart the runtime in the menu under \"runtime\" --> \"restart runtime\". Then, you must run the cell in chapter 0 and the code cell below again before moving on to section 4.1."
      ],
      "metadata": {
        "id": "QM5C-YJwWULU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0XA23gxbCbt"
      },
      "outputs": [],
      "source": [
        "# Install the most recent unbabel-comet package\n",
        "!pip install unbabel-comet==1.1.3\n",
        "\n",
        "# In case this is outdated, check out the newest version or simply uncomment and install the line below\n",
        "#!pip install unbabel-comet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v10ysctSlzd"
      },
      "source": [
        "## 4.1 Calculating an average COMETinho score for an entire text\n",
        "\n",
        "Again we have `NMT_hypothesis` as the machine-translated text and `HT_reference` as the human-translated text. In contrast to the three metrics above, we now also need a source document, so we'll now also indicate the source text `source` in our code cell below.\n",
        "\n",
        "Now we can calculate an average COMETinho score for the entire *NMT_hypthesis* text. We use the same COMETinho Estimator model (`eamt22-cometinho-da`), as in our [Advanced MT Quality Evaluation notebook](https://colab.research.google.com/drive/1UgsqgN-6yfDESU7Geei4RXzJShEQNViZ?usp=sharing). Simply run the code cell below (this may take a while)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RgzfvYdKjOQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f652677-8b36-44be-84f8-bc7ce65ffbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-27 11:45:46.441223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-27 11:45:48.033826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 11:45:48.034093: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 11:45:48.034138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Global seed set to 12\n",
            "eamt22-cometinho-da.tar.gz: 307MB [00:08, 36.8MB/s]               \n",
            "Downloading (…)okenizer_config.json: 100% 2.00/2.00 [00:00<00:00, 178B/s]\n",
            "Downloading (…)lve/main/config.json: 100% 430/430 [00:00<00:00, 60.4kB/s]\n",
            "Downloading (…)tencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 10.1MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 150/150 [00:00<00:00, 55.1kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 471M/471M [00:12<00:00, 36.3MB/s]\n",
            "WARNING:comet.models.base:Path lightning_logs/cometinho_part-i/checkpoints/epoch=0-step=899999.ckpt does not exist!\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Predicting DataLoader 0: 100% 2/2 [00:01<00:00,  1.08it/s]\n",
            "NMT_hypothesis\tSegment 0\tscore: 0.2897\n",
            "NMT_hypothesis\tSegment 1\tscore: 1.6138\n",
            "NMT_hypothesis\tSegment 2\tscore: 0.5832\n",
            "NMT_hypothesis\tSegment 3\tscore: 0.9947\n",
            "NMT_hypothesis\tSegment 4\tscore: 0.6846\n",
            "NMT_hypothesis\tSegment 5\tscore: 0.3991\n",
            "NMT_hypothesis\tSegment 6\tscore: 0.6485\n",
            "NMT_hypothesis\tSegment 7\tscore: 0.3827\n",
            "NMT_hypothesis\tSegment 8\tscore: 0.6480\n",
            "NMT_hypothesis\tSegment 9\tscore: 0.4723\n",
            "NMT_hypothesis\tSegment 10\tscore: 0.7083\n",
            "NMT_hypothesis\tSegment 11\tscore: 0.0592\n",
            "NMT_hypothesis\tSegment 12\tscore: 0.9182\n",
            "NMT_hypothesis\tSegment 13\tscore: 0.0841\n",
            "NMT_hypothesis\tSegment 14\tscore: 0.5831\n",
            "NMT_hypothesis\tSegment 15\tscore: 0.6738\n",
            "NMT_hypothesis\tscore: 0.6089\n"
          ]
        }
      ],
      "source": [
        "# Calculating the COMETinho Score\n",
        "!comet-score -s source -t NMT_hypothesis -r HT_reference --model eamt22-cometinho-da --gpu 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average COMETinho score calculated for the entire *NMT_hypothesis* text is shown at the bottom (*NMT_hypothesis score*) and should be **0.6089**. Above this average score, you can see a individual scores for each segment (each sentence in the text files). Counting starts at 0 and ends at 15, representing the 16 lines in each of our text files. Our sentence COMETinho scores range from 0.0592 to 1.6138."
      ],
      "metadata": {
        "id": "tdl_YwqEHqUC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW4PSxti-m0"
      },
      "source": [
        "## 4.2 Calculating an average COMET Quality Estimation score for an entire text\n",
        "\n",
        "Below, we load the COMET Quality Estimation (QE) model (`wmt21-comet-qe-mqm`) and calculate a quality estimation score is calculated for the hypothesis and source texts (without access to the reference text). Again, calculating this document-level score may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jXur6wZSjBLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92ef017-1530-44bc-c126-6c221d477e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-27 11:46:55.579046: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-27 11:46:56.780095: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 11:46:56.780211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 11:46:56.780228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Global seed set to 12\n",
            "wmt21-comet-qe-mqm.tar.gz: 1.72GB [00:44, 38.5MB/s]                \n",
            "Downloading (…)tencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 10.0MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 616/616 [00:00<00:00, 87.2kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 2.24G/2.24G [00:13<00:00, 170MB/s]\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Predicting DataLoader 0: 100% 2/2 [00:16<00:00,  8.06s/it]\n",
            "NMT_hypothesis\tSegment 0\tscore: 0.1008\n",
            "NMT_hypothesis\tSegment 1\tscore: 0.1321\n",
            "NMT_hypothesis\tSegment 2\tscore: 0.1265\n",
            "NMT_hypothesis\tSegment 3\tscore: 0.1510\n",
            "NMT_hypothesis\tSegment 4\tscore: 0.1589\n",
            "NMT_hypothesis\tSegment 5\tscore: 0.1078\n",
            "NMT_hypothesis\tSegment 6\tscore: 0.1348\n",
            "NMT_hypothesis\tSegment 7\tscore: 0.1126\n",
            "NMT_hypothesis\tSegment 8\tscore: 0.1449\n",
            "NMT_hypothesis\tSegment 9\tscore: 0.1173\n",
            "NMT_hypothesis\tSegment 10\tscore: 0.1216\n",
            "NMT_hypothesis\tSegment 11\tscore: 0.0543\n",
            "NMT_hypothesis\tSegment 12\tscore: 0.1146\n",
            "NMT_hypothesis\tSegment 13\tscore: 0.1099\n",
            "NMT_hypothesis\tSegment 14\tscore: 0.1108\n",
            "NMT_hypothesis\tSegment 15\tscore: 0.1347\n",
            "NMT_hypothesis\tscore: 0.1208\n"
          ]
        }
      ],
      "source": [
        "# Calculating the Quality Estimation Score\n",
        "!comet-score -s source -t NMT_hypothesis --model wmt21-comet-qe-mqm --gpu 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average COMETinho (QE) score calculated for the entire *NMT_hypothesis* text is **0.1208**. Again, individual segment scores are shown above the average score. The segment scores range from 0.0543 to 1.1589."
      ],
      "metadata": {
        "id": "5RdsISobJSAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now successfully calculated various automatic MT quality evaluation metrics for entire texts (at document level). You are now well prepared to evaluate the quality of MT systems and of machine-translated texts, which is becoming more and more relevant in the professional industry. You could also perform an automatic quality evaluation on your own machine translations created with your own NMT model, which you can train in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "MHzcky_CiqDB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}