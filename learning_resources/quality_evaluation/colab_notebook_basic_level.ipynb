{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITMK/DataLitMT/blob/main/colab_notebook_basic_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![KeyVisual_DataLitMT - Kopie.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "# Data Evaluation – Basic Level\n",
        "**MT Quality Score Calculator for Metrics Based on String Matching and for Embedding-based Metrics**"
      ],
      "metadata": {
        "id": "OaHdVJZpHWys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This is a Colab notebook for calculating automatic metrics for machine translation (MT) quality evaluation, which covers the *Data Evaluation* dimension of the [DataLit<sup>MT</sup> Competence Matrix](https://itmk.github.io/The-DataLitMT-Project/matrix/) and here particularly the subdimensions of *Data Analysis*, *Data Verbalisation* (to some extent) and *Data Interpretation*. Given that MT systems are increasingly used as productivity-enhancing tools in the professional translation process, both manual and automatic evaluation of MT output become more and more important. Automatic MT quality evaluation is part of *Technical MT Literacy*, as illustrated in the [Professional Machine Translation Literacy Framework](https://itmk.github.io/The-DataLitMT-Project/framework/#professional-mt-literacy). In this notebook, we will introduce two traditional MT quality metrics based on string matching and two modern embedding-based metrics, and we will illustrate the differences between these metrics. After working through this notebook, you should have a basic understanding of how automatic MT quality evaluation works and how the various scores differ from each other.\n",
        "\n",
        "The code used for the score calculations in this notebook is adopted from the Natural Language Toolkit ([NLTK](https://www.nltk.org/)) and the respective GitHub repositories of each metric (references will be given throughout the notebook).\n",
        "\n",
        "All metrics presented in this notebook are based on a comparison between a machine-translated sentence (which is often called *hypothesis* in MT research) and a corresponding human reference translation (*reference*), which is the 'gold standard' that any MT hypothesis should strive to achieve. The fundamental distinction we make in this notebook is between traditional MT quality scores which rely on exact string matching between string A (MT output/hypothesis) and string B (reference translation/gold standard) to calculate the similarity or dissimilarity between the two strings, and embedding-based metrics, which use the concept of *word embeddings* to calculate the (dis)similarity between hypothesis and reference based on the relative proximity of their word embeddings in high-dimensional vector space. If you’re confused right now, don’t worry, we will explain all of this in more detail in the course of this notebook."
      ],
      "metadata": {
        "id": "AN8d0biYKttO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 Housekeeping\n",
        "\n",
        "You can run all cells in this notebook without having to change any of the code. However, in the examples discussed below, feel free to enter your own MT and reference sentences, if you'd like to calculate individual scores.\n",
        "\n",
        "First, we need to ensure that we installed/upgraded the Python installer *pip*, which we will use to install external libraries in our notebook. To do so, run the cell below."
      ],
      "metadata": {
        "id": "giCwQH7e9sQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade to the current version of pip\n",
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "id": "NyqBfxnx6hqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c2fffa-9eca-45fa-a4c2-3612bf0d6ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (23.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s it, you’re ready to go!"
      ],
      "metadata": {
        "id": "QswDXF2qCGS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# String-based Metrics"
      ],
      "metadata": {
        "id": "E5-WBIy5B-T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we cover traditional MT quality evaluation metrics which are based on the comparison of two strings, i.e., the MT output/hypothesis and the human reference translation. A full understanding of these string matching-based metrics relies on understanding the three basic concepts of *precision*, *recall* and *n-gram*. In order to avoid overcrowding the current notebook, we created [a companion notebook for automatic MT quality evaluation metrics based on string matching](https://colab.research.google.com/drive/1G0uwf5kPG5d5kmd-LbvkkssKoza9N622?usp=sharing), which covers these three concepts in detail. You should get a basic understanding of traditional MT quality metrics without working through this companion notebook. However, if you want to get down to the nuts and bolts of these metrics, the companion notebook is the place to start."
      ],
      "metadata": {
        "id": "Zjc16TYWCJzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 BLEU (a similarity measure)\n",
        "\n",
        "BLEU is an acronym for **B**i**L**ingual **E**valuation **U**nderstudy. It was originally proposed in [Papineni et al. (2002): BLEU: A Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf). BLEU is still a commonly used automatic MT quality score because it is widely known in MT research and practice and it often achieves reasonably good correlations with human quality judgements. (A high correlation with human quality judgements is considered the ultimate ‘seal of approval’ for any automatic MT quality evaluation metric.)\n",
        "\n",
        "BLEU is a similarity measure, which derives its name from the fact that it measures the degree of similarity between two strings (i.e., the degree of similarity between the machine-translated sentence and the human-translated reference sentence). The closer the machine translation is to the reference translation, the better is the quality of the MT output (remember that we consider the reference to be our gold standard). Therefore, the *higher* the BLEU score calculated for a given MT output/reference pair, the *higher* the quality of the MT output is assumed to be, and vice versa.\n",
        "\n",
        "BLEU scores published in official machine translation competitions such as the [Conference on Machine Translation (WMT)](http://www.statmt.org/wmt20/) are calculated using the [SacreBLEU](https://github.com/mjpost/sacrebleu) package. SacreBLEU is a standardised method for BLEU score calculation allowing the comparison of BLEU scores provided by different MT developers for different MT systems. sacreBLEU was originally proposed in [Post (2018): A Call for Clarity in Reporting BLEU Scores](https://arxiv.org/abs/1804.08771). To follow good industry practice, we also use sacreBLEU in this notebook."
      ],
      "metadata": {
        "id": "ZHoqS9kl-3G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0 Installing sacreBLEU\n",
        "\n",
        "Run the cell below to install sacreBLEU."
      ],
      "metadata": {
        "id": "LfxlQwReReTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sacreBLEU functions\n",
        "!pip install sacrebleu\n",
        "import sacrebleu"
      ],
      "metadata": {
        "id": "iZXrmuZQCT59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Short Example\n",
        "\n",
        "In the example below, we define a human reference sentence and a machine-translated hypothesis. For these two sentences (which we’ll encounter at various points in this notebook), we can calculate a BLEU score. Run the following cell to do so."
      ],
      "metadata": {
        "id": "MP_pf8P9nAKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define reference and hypothesis\n",
        "hypothesis_sacreBLEU = ['I drove the automobile to the gasoline station'] \n",
        "reference_sacreBLEU = ['I drove the car to the gas station']                                                                                                                                                 \n",
        "\n",
        "# Calculate and print sacreBLEU score\n",
        "sacrebleu.corpus_bleu(hypothesis_sacreBLEU, [reference_sacreBLEU])"
      ],
      "metadata": {
        "id": "JP6AGRWf-411"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we get a BLEU score of 27.05 (indicated by *BLEU = 27.05*). The scale here ranges from 0 to 100, so 27.05 is a below-average value (note that the two sentences express basically the same meaning, albeit using slightly different words). In real-life MT quality evaluation scenarios, BLEU scores are rarely as high as 100 (which would mean that the MT output is identical to the reference translation) and usually start being competitive from 20 upward. We will ignore the additional values on the left side of the BLEU score. These will be covered in our [Advanced MT Quality Evaluation notebook](https://colab.research.google.com/drive/1UgsqgN-6yfDESU7Geei4RXzJShEQNViZ?usp=sharing), which discusses the various MT quality metrics illustrated here in much more detail."
      ],
      "metadata": {
        "id": "BeIe8XvX--EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2 Your Own Example\n",
        "\n",
        "If you would like to test BLEU score calculation with an example of your own, run the first code cell below and then type in your own example sentences in the field below the cell (both times, confirm with Enter). Then, run the second cell below in order to calculate the BLEU score for your sentences."
      ],
      "metadata": {
        "id": "U-cMBM0vnCjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter machine-translated sentence (hypothesis) and the human reference translation (reference)\n",
        "own_hypothesis = input(\"\\nEnter the machine-translated sentence here: \")\n",
        "own_reference = input(\"\\nEnter the human-translated sentence here: \")"
      ],
      "metadata": {
        "id": "5O_OqusknG0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print your own sacreBLEU score\n",
        "sacrebleu.corpus_bleu([own_hypothesis], [[own_reference]])"
      ],
      "metadata": {
        "id": "28o4hYpQnSxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can always come back here to calculate further BLEU scores."
      ],
      "metadata": {
        "id": "XeHtE-8cFcXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 TER\n",
        "\n",
        "In addition to string-based *similarity* measures, there are also string-based *distance* measures. These derive their name from the fact that they measure the degree of *dissimilarity* or distance between two strings. Accordingly, the *higher* the score calculated by these metrics, the *lower* the quality of the MT output is assumed to be, and vice versa (opposite to the scores of similarity measures such as BLEU).\n",
        "\n",
        "**TER** is an acronym for **T**ranslation **E**dit **R**ate, originally proposed in [Snover et al. (2006): A Study of Translation Edit Rate with Targeted Human Annotation](http://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf)."
      ],
      "metadata": {
        "id": "3FpqLQEKzHj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0 Installing TER\n",
        "\n",
        "For calculating TER, we need to install the [pyter](https://pypi.org/project/pyter3/) and import some NLTK functions. This is done by running the code below."
      ],
      "metadata": {
        "id": "hr2_p6o40TwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the current version of the pyter package\n",
        "!pip install --upgrade pyter3\n",
        "\n",
        "# Import ter() and word_tokenize() functions\n",
        "from pyter import ter\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZM9KNEjEzJU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Short Example\n",
        "\n",
        "In the example below, we define the same reference and hypothesis sentences we used for our BLEU score calculation. For these two sentences, we can then calculate a TER score. Run the following cell to calculate the score."
      ],
      "metadata": {
        "id": "0XgQNGOP0Mfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define reference and hypothesis\n",
        "hypothesis_ter = 'I drove the automobile to the gasoline station'\n",
        "reference_ter = 'I drove the car to the gas station'\n",
        "\n",
        "# Calculate and print Translation Edit Rate\n",
        "TER = ter(word_tokenize(hypothesis_ter), word_tokenize(reference_ter))\n",
        "print(f\"TER: {TER}\")"
      ],
      "metadata": {
        "id": "VyGZmrtr0Of9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we obtain a TER score of 0.25 (with the scale of possible scores ranging from 0 to 1). In other words, our editing effort amounted to 25 %, which is rather low, indicating a good quality of the MT output (remember, the lower the TER score, the higher and MT quality and vice versa). Note that our BLEU score for our hypothesis/reference pair indicated a rather poor MT quality (27.05 of 100, with BLEU being a similarity measure -> the higher the score the better the MT quality and vice versa), while our TER score indicates a good MT quality. This is a good lesson to learn: Automatic MT quality scores should never be taken at face value. Always ask yourself how the metric for which the score was calculated actually works and *what the score means in relation to the operating principle of this metric!*\n",
        "\n",
        "TER or similar distance measures are often used in the translation industry for calculating the post-editing effort involved in MT-assisted translation projects (which can then be used for price calculation). For more information on this topic, see, e.g. the article by [Kirchner (2020)](https://aclanthology.org/2020.eamt-1.38/)."
      ],
      "metadata": {
        "id": "Gpta-A3xGukd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Your Own Example\n",
        "\n",
        "If you would like to test TER score calculation with an example of your own, run the first code cell below and then type in your own example sentences in the field below the cell (both times, confirm with Enter). Then, run the second cell below in order to calculate the TER score for your sentences (the same as under 1.2)."
      ],
      "metadata": {
        "id": "E04o2Nss07VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter machine-translated sentence (hypothesis) and the human reference translation (reference)\n",
        "own_ter_hypothesis = input(\"\\nEnter the machine-translated sentence here: \")\n",
        "own_ter_reference = input(\"\\nEnter the human-translated sentence here: \")"
      ],
      "metadata": {
        "id": "81wYgR_S095Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print your own Translation Edit Rate\n",
        "TER = ter(word_tokenize(own_ter_hypothesis), word_tokenize(own_ter_reference))\n",
        "print(f\"TER: {TER}\")"
      ],
      "metadata": {
        "id": "ZY7_dIBQSNgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, feel free to come back here any time to calculate further TER scores."
      ],
      "metadata": {
        "id": "i_bmYjykHIzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding-based Metrics BERTScore and COMET\n",
        "\n",
        "Now that we are familiar with traditional MT quality metrics based on string matching, we can move on to modern quality metrics which are based on embeddings. These embedding-based metrics are a bit more complex than the traditional string matching-based metrics, because they rely on the concept of *word embeddings*. If you want to know more about this concept, have a look at our [companion notebook for embedding-based MT quality metrics](https://colab.research.google.com/drive/1qM9srwtTfaapTnKcQGMYct1iFGyL1XjB?usp=sharing). If you want to skip this notebook, just bear in mind that word embeddings offer a way to compare the *semantic* similarity between two words, instead of just their *formal* similarity (as string-based metrics such as BLEU and TER do). For example, string-based metrics would consider the two words *car* and *automobile* to be dissimilar because they differ in their surface forms. Embedding-based metrics, on the other hand, would consider the two words to be similar because they basically express the same meaning. This distinction should become clearer in the following sections."
      ],
      "metadata": {
        "id": "XuETdpEGB52t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DNPdx14nQ28"
      },
      "source": [
        "## 3 BERTScore: semantic similarity\n",
        "\n",
        "BERTScore is an embedding-based metric relying on semantic similarity. BERTScore was originally proposed in [Zhang et al. (2019): BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675). It uses the pre-trained word embeddings from Google's neural language model BERT (see [Devlin et al. (2019): BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)) in order to compare the similarity of words in hypothesis and reference sentences.\n",
        "Let’s calculate our own BERTScores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFXUUb3Rwh-U"
      },
      "source": [
        "### 3.0 Installing BERTScore\n",
        "\n",
        "First, we need to install the packages required to compute BERTScore. BERTScore is implemented as part of the bert-score package available in the [official BERTScore GitHub repository](https://github.com/Tiiiger/bert_score). Since embedding-based metrics rely on large embedding models, installing these metrics will usually take considerably longer than installing and importing traditional MT quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install BERTScore\n",
        "!pip install bert-score\n",
        "from bert_score import score"
      ],
      "metadata": {
        "id": "I8orfbTKXqtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Short Example\n",
        "Run the following cells to calculate BERTScore for the same hypothesis-reference pair we used for our BLEU and TER examples."
      ],
      "metadata": {
        "id": "ofW5yw_j4Y7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hypothesis and reference\n",
        "hypothesis = ['I drove the automobile to the gasoline station']\n",
        "reference = ['I drove the car to the gas station']"
      ],
      "metadata": {
        "id": "4FYWY6IoTgn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1oBerglbfSU"
      },
      "source": [
        "# Calculate BERTScore\n",
        "P, R, F1 = score(hypothesis, reference, lang=\"en\", rescale_with_baseline=True, verbose=True)\n",
        "# Note here, the language we are working with is English (lang=\"en\"). Change this to lang=\"de\" when working with German sentences, for example.\n",
        "\n",
        "# Print Rbert, Pbert and Fbert\n",
        "print(f\"\\nBERTScore-Recall: {R}\\n\")\n",
        "print(f\"BERTScore-Precision: {P}\\n\")\n",
        "print(f\"BERTScore-F-Measure: {F1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty39-mFynOBk"
      },
      "source": [
        "The cell above produces several scores, but we can just focus on $F_{BERT}$, which is the final BERTScore for this hypothesis-reference pair. BERTScore rescales this output so that it lies in a range between 0 and 1, with 0 being a very poor and 1 being a very good score (all differences aside, BERTScore is basically a similarity measure, just like BLEU). As you can see, our hypothesis-reference pair is scored very highly (BERTScore-F-Measure = 0.8595) because the two sentences express almost the same meaning, although they differ in their respective surface forms. Remember that our string matching-based metric BLEU scored the same hypothesis-reference pair rather poorly. This illustrates again the fundamental difference between traditional metrics based on string matching and modern embedding-based metrics: String-based metrics can only capture formal similarities between two strings whereas embedding-based metrics can capture semantic similarities between two strings, regardless of any formal differences between them. Keep this important difference in mind for future MT quality evaluation scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Your own Example. \n",
        "\n",
        "You can type in your own English hypothesis and reference sentences to calculate a BERTScore (same as above).\n",
        "BERTScore covers a large range of languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages). Feel free to change hypothesis and reference according to your requirements. Note: For languages other than English, the command *lang=\"en\"* has to be changed to the relevant input language."
      ],
      "metadata": {
        "id": "_n5CXO9LyI1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter machine-translated sentence (hypothesis) and the human reference translation (reference)\n",
        "own_BERT_hypothesis = input(\"\\nEnter the machine-translated sentence here: \")\n",
        "own_BERT_reference = input(\"\\nEnter the human-translated sentence here: \")"
      ],
      "metadata": {
        "id": "PvonpCwtyGPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BERTScore\n",
        "P, R, F1 = score([own_BERT_hypothesis], [own_BERT_reference], lang=\"en\", rescale_with_baseline=True, verbose=True)\n",
        "# Remember to change the language lang=\"en\" if you input a non-english sentence (let's say for German: lang=\"de\")\n",
        "\n",
        "# Print Rbert, Pbert and Fbert\n",
        "print(f\"\\nBERTScore-Recall: {R}\\n\")\n",
        "print(f\"BERTScore-Precision: {P}\\n\")\n",
        "print(f\"BERTScore-F-Measure: {F1}\")"
      ],
      "metadata": {
        "id": "wbogagKSyALz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may want to come up with more examples of hypothesis/reference pairs which differ in their form but express the same meaning or vice versa to see how these examples will be scored differently by string-based and embedding-based metrics respectively."
      ],
      "metadata": {
        "id": "G4bJvh_Fx5D-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfNrPeYnBdj"
      },
      "source": [
        "## 4 COMET: semantic similarity + human quality judgements\n",
        "\n",
        "COMET is an embedding-based metric relying on semantic similarity and trained additionally on human quality judgements. **COMET** is an acronym for **C**rosslingual **O**ptimized **M**etric for **E**valuation of **T**ranslation. It was originally proposed  in [Rei et al. (2020): COMET: A Neural Framework for MT Evaluation](https://www.aclweb.org/anthology/2020.emnlp-main.213/). In contrast to traditional MT quality metrics based on string matching (such as BLEU or TER) and in contrast to the embedding-based metric BERTScore, COMET uses not only the machine-translated hypothesis and the human-translated reference but also the original source sentence. COMET is then able to measure the interlingual semantic similarity between these three sentences, i.e., COMET can establish both the semantic similarity between hypothesis and reference as well as the semantic similarity between source and hypothesis/reference. In addition, COMET takes into consideration prior human quality judgements taken, for example, from a human error annotation of machine-translated sentences or from a human ranking of such sentences on a 100-point-ranking scale. These human quality judgements allow COMET to measure nuanced aspects of machine-translated sentences beyond semantic similarity to their source/reference sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd8NEhhSwpD_"
      },
      "source": [
        "### 4.0 Installing COMET\n",
        "First, we need to install the packages required to compute COMET scores. The various COMET models are implemented as part of the unbabel-comet package available in the official [COMET GitHub repository](https://github.com/Unbabel/COMET). Since embedding-based metrics rely on large embedding models, installing COMET and loading the model will usually take some time (same as with BERTScore).\n",
        "\n",
        "Note: Running this cell might result in the notebook requesting you to restart the runtime: \"You must restart the runtime in order to use the newly installed version\". You can restart the runtime in the menu under \"runtime\" --> \"restart runtime\". Then, you must run the cell in chapter 0 and the cell below again before moving on to section 4.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0XA23gxbCbt"
      },
      "source": [
        "# Install the most recent unbabel-comet package\n",
        "!pip install unbabel-comet==1.1.3\n",
        "\n",
        "# In case this is outdated, check out the newest version or simply uncomment and install the line below\n",
        "#!pip install unbabel-comet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Lightweight COMETinho\n",
        "\n",
        "One of the most recent and efficient models is COMETinho ([Rei et al. (2022): Searching for COMETinho: The Little Metric That Could](https://aclanthology.org/2022.eamt-1.9/)). We also use COMETinho in this notebook, as it is a streamlined 'lightweight' version of the original COMET, which offers basically the same performance as the bigger COMET models, but at a much faster speed (meaning that it loads much faster). Run the cell below to load the COMETinho model."
      ],
      "metadata": {
        "id": "ngafA_KF7eiJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3n3-lVZczLW"
      },
      "source": [
        "# Loading the COMETinho 'eamt22-cometinho-da' model\n",
        "from comet import download_model, load_from_checkpoint\n",
        "model_path_inho = download_model(\"eamt22-cometinho-da\")\n",
        "model_inho = load_from_checkpoint(model_path_inho)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1 Short Example Using Single Sentences\n",
        "\n",
        "The cell below calculates a COMETinho score for one hypothesis. Note that we do not only have to specify the human reference translation, but also the source sentence (we deviate here from our previous example sentences, as COMET is a different beast than the other MT quality metrics we covered in this notebook)."
      ],
      "metadata": {
        "id": "43juFG1f_bjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining source, hypothesis and reference sentences\n",
        "data = [{ \"src\": \"If you do not agree to these Conditions do not use this website.\",\n",
        "          \"mt\": \"Wenn Sie mit diesen Bedingungen nicht einverstanden sind, benutzen Sie diese Website nicht.\",\n",
        "          \"ref\": \"Wenn Sie mit diesen Bedingungen nicht einverstanden sind, sehen Sie von einer Nutzung dieser Website ab.\"      }]\n",
        "\n",
        "# Calculate COMET scores\n",
        "seg_scores, sys_score = model_inho.predict(data, batch_size=8, gpus=0) # This can be changed to 'gpus=1' if GPU is available\n",
        "print(\"\\nSegment scores:\", seg_scores) # for multiple sentences\n",
        "print(\"COMETinho score:\", sys_score)"
      ],
      "metadata": {
        "id": "Yv4Sbn-D71lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we obtain a COMETinho score of around 0.68 for our hypothesis/reference/source combination. Above the COMETinho value, you can see a segment score, which indicates the COMETinho score per sentence (= per segment). Since we only compute a score for one hypothesis in this example, the sentence score is identical to the overall COMETinho score. If you calculate a score for multiple sentences (as we do below), you would have multiple segment scores, and one overall COMETinho score. COMET(inho) scores are more difficult to interpret than the previous scores we covered in this notebook, since individual scores will have to be interpreted within the overall range of scores that were obtained using the respective COMET(inho) model in larger MT quality evaluation campaigns. So, scores such as 0.68 do not tell us anything by themselves but have to be interpreted relative to a COMET model’s score range. More information on this can be found in the [official COMET Documentation](https://unbabel.github.io/COMET/html/faqs.html#is-there-a-theoretical-range-of-values-for-the-comet-regressor) (the documentation does not provide any specific information on the COMETinho model we used in our example, so we don’t quite know whether 0.68 is a good or a bad score). However, we can use two COMETinho scores to measure which of the two (or more) MT hypotheses is the better one. This is what we do in the example below."
      ],
      "metadata": {
        "id": "w5Zm9wxvl5br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Another Example Using Multiple Sentences\n",
        "As mentioned above, you could also calculate an average COMETinho score for multiple sentences, with a different value for each segment (sentence). This is illustrated below."
      ],
      "metadata": {
        "id": "pAANQTR-jYOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining source, hypothesis and reference sentences\n",
        "data = [{ \"src\": \"If you do not agree to these Conditions do not use this website.\",\n",
        "          \"mt\": \"Wenn Sie mit diesen Bedingungen nicht einverstanden sind, benutzen Sie diese Website nicht.\",\n",
        "          \"ref\": \"Wenn Sie mit diesen Bedingungen nicht einverstanden sind, sehen Sie von einer Nutzung dieser Website ab.\"      },\n",
        "        {\"src\": \"Please read these Conditions; they are important.\",\n",
        "         \"mt\": \"Bitte lesen Sie diese Bedingungen; sie sind wichtig.\",\n",
        "         \"ref\": \"Bitte lesen Sie diese Bedingungen durch; sie sind wichtig.\"        } ]\n",
        "\n",
        "# Calculate COMET scores\n",
        "seg_scores, sys_score = model_inho.predict(data, batch_size=8, gpus=0) # This can be changed to 'gpus=1' if GPU is available\n",
        "print(\"\\nSegment scores:\", seg_scores) # for multiple sentences\n",
        "print(\"COMETinho score:\", sys_score)"
      ],
      "metadata": {
        "id": "lBbTANs5jT6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we score two MT hypotheses and obtain two segment/sentence scores of 0.68 (for the first segment/sentence, as in the example above) and of 0.99 (for the second segment/sentence). The final COMETinho score of 0.83 is simply the average of the two segment/sentence scores. Again, we do not quite know what to make of the final score, but we can see that COMETinho scores the second segment (where MT hypothesis and reference are identical) much higher than the first segment. You could also use this COMETinho model to score segments where source and reference are identical and where only the MT hypothesis differs. This way, you could check which hypothesis COMETinho considers to be the better one."
      ],
      "metadata": {
        "id": "2grcxikfmEUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now successfully calculated various automatic MT quality evaluation metrics and should now have a basic understanding of how the individual scores work, how they are interpreted and what their individual advantages and shortcomings are. If you want to delve deeper into the topic of automatic MT quality evaluation, have a look at our [Advanced MT Quality Evaluation notebook](https://colab.research.google.com/drive/1UgsqgN-6yfDESU7Geei4RXzJShEQNViZ?usp=sharing). If you would like to evaluate the quality of an entire document, you can use our [companion notebook\n",
        "on quality evaluation at document level](https://colab.research.google.com/drive/19a896dbRBVtJmqA6JIFBU7bUiASfAo6V?usp=sharing) to calculate an average metric for an entire document. You could also use your knowledge to perform an automatic quality evaluation of your own NMT model, which you can train in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "L5dnSey0jT21"
      }
    }
  ]
}
