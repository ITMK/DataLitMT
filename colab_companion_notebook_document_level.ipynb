{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITMK/DataLitMT/blob/main/colab_companion_notebook_document_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LntKRoYSHhe"
      },
      "source": [
        "![KeyVisual_DataLitMT - Kopie.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "# Data Evaluation – Companion Notebook for MT Quality Evaluation at Document Level\n",
        "**MT Quality Score Calculator at Document Level for Metrics Based on String Matching and for Embedding-based Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3FxGjAG7E-_"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is a companion notebook for our [Basic](https://colab.research.google.com/drive/1KY2qGmDoJOSOPewseVszL0u8J-x7xa7I?usp=sharing) and [Advanced](https://colab.research.google.com/drive/1UgsqgN-6yfDESU7Geei4RXzJShEQNViZ?usp=sharing) MT Quality Evaluation notebooks, which allows you to calculate MT quality scores for an entire text. We implement this document-level evaluation for the four metrics covered in our main notebooks, i.e., BLEU and TER, BERTScore and COMET. \n",
        "\n",
        "The code used for these calculations is adopted largely from the Natural Language Toolkit ([NLTK](https://www.nltk.org/)) and the respective GitHub repositories of each metric, as referenced in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giCwQH7e9sQg"
      },
      "source": [
        "## 0 Housekeeping\n",
        "\n",
        "You can run all cells in this notebook without having to change any of the code.\n",
        "However, in the examples discussed below, feel free to indicate your own (text) files, if you'd like to calculate individual scores. In our tutorial video, we will show you what you need to do if you want to deviate from the workflow illustrated in this video to calculate document-level scores for your own texts.\n",
        "\n",
        "We first need to ensure that we installed/upgraded pip. To do so, run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyqBfxnx6hqp"
      },
      "outputs": [],
      "source": [
        "# Upgrade to the current version of pip\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Your Own Documents\n",
        "\n",
        "If you would like to know how to upload your own documents to calculate the metrics covered in this notebook, you can look at the accompanying tutorial video that will also guide you through that process at this step."
      ],
      "metadata": {
        "id": "13DEDumNqxdm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nyqNHa8QOR8"
      },
      "source": [
        "## Clone our GitHub Repository\n",
        "\n",
        "First, we need to clone our [DataLit<sup>MT</sup> GitHub Repository](https://github.com/ITMK/DataLitMT/tree/main) to access the three text files we'll be working with in this notebook. Simply run the  code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6khzt48QSw0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ITMK/DataLitMT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPX-gEs-ZfQX"
      },
      "source": [
        "Let's now change the directory directly to the folder that contains the three files so we can easily refer to the files throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-R9r3c5ZWoD"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/DataLitMT/learning_resources/quality_evaluation/example_texts/\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files we use in this notebook are taken from our [Data Evaluation notebook on machine translationese and post-editese](https://colab.research.google.com/drive/1H_Nn-TRbOJlGPp2tmcJ_EAdIhlDC-xVK?usp=sharing). Ideally, have a look at these files to see what you will be working with. On the left-hand-side of this notebook, there is an icon indicating a folder with your files. By clicking on the three vertical dots on the right of the files of `NMT_hypothesis`, `HT_reference` and `source`, you can download these and open them locally on your desktop. These steps will also be shown in our tutorial video.\n",
        "\n",
        "To ensure the files are of the same length (the documents need to have an exact same number of sentences), you can run the code cell below. Each file should have a length of 16 (16 sentences). Having the same length is required to calculate automatic quality metrics at document level, so checking the length of the files is good practice."
      ],
      "metadata": {
        "id": "zeGKjWxWM0vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('HT_reference', 'r') as reader:\n",
        "  reference = reader.readlines()\n",
        "\n",
        "with open('NMT_hypothesis', 'r') as reader:\n",
        "  hypothesis = reader.readlines()\n",
        "\n",
        "with open('source', 'r') as reader:\n",
        "  source = reader.readlines()\n",
        "\n",
        "print('Length of reference', len(reference))\n",
        "print('Length of hypothesis', len(hypothesis))\n",
        "print('Length of source', len(source))"
      ],
      "metadata": {
        "id": "sN11RQg_EFxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHoqS9kl-3G-"
      },
      "source": [
        "# 1 BLEU (sacreBLEU implementation)\n",
        "\n",
        "Here, we calculate document-level BLEU scores using the [sacreBLEU implementation](https://github.com/mjpost/sacrebleu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF9AIabIM23R"
      },
      "source": [
        "## 1.0 Importing Necessary Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZXrmuZQCT59"
      },
      "outputs": [],
      "source": [
        "# Import sacreBLEU functions\n",
        "!pip install sacrebleu\n",
        "import sacrebleu\n",
        "from sacrebleu.metrics import BLEU, TER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwbGKCzX_K5u"
      },
      "source": [
        "## 1.1 Calculating an average sacreBLEU score for an entire text\n",
        "\n",
        "We have the machine-translated text file `NMT_hypothesis` and the human-translated reference text `HT_reference`. Simply run the code cell below to calculate an average sacreBLEU score for the entire text.\n",
        "\n",
        "Note: In the code cell below, `bleu` indicates that we want to calculate the BLEU score. The sacreBLEU package also allows calculating other metrics such as TER (as shown below). `--score-only` (could also be written as `-b`) indicates that we only want the actual score to be printed (i.e., we want no detailed information on score calculation). If you are interested in the details, feel free to delete `--score-only` in the cell below. Similarly, `--width 2` (could also be written as `--w`) indicates that we only want to have two values after the decimal point. This number could be changed or `--width 2` could be removed to show only one value after the decimal point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate an average sacreBLEU score for the entire text file\n",
        "!sacrebleu HT_reference -i NMT_hypothesis -m bleu --score-only --width 2"
      ],
      "metadata": {
        "id": "83u3bE4-D8bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything went well, we should get an average sacreBLEU score for the *NMT_hypothesis* text of **34.69**. For information on how to interpret this score and the other scores calculated in this notebook, see our main notebooks on MT quality evaluation."
      ],
      "metadata": {
        "id": "fTNaOBsTU6rw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba_0iuqJ1V24"
      },
      "source": [
        "##2 TER\n",
        "\n",
        "Here, we calculate document-level TER. To do so, we can also use the [sacreBLEU package](https://github.com/mjpost/sacreBLEU#ter) installed above. In our BLEU calculation above, we indicated `bleu` in the code cell. Using the same sacreBLEU package, we can simply indicate `ter` in the code cell below to calculate an average TER score for the entire text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSm1BgW91kj4"
      },
      "source": [
        "## 2.1 Calculating an average TER score for an entire text\n",
        "\n",
        "We have the machine-translated text file `NMT_hypothesis` and the human-translated reference text `HT_reference`. Simply run the code cell below to calculate an average TER score for the entire text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TER\n",
        "!sacrebleu HT_reference -i NMT_hypothesis -m ter --score-only --width 2"
      ],
      "metadata": {
        "id": "3Rseao2bJPX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, if everything went well, we should get an average TER score for the *NMT_hypothesis* text of **56.80**."
      ],
      "metadata": {
        "id": "BJdy50twVBMc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DNPdx14nQ28"
      },
      "source": [
        "# 3 BERTScore\n",
        "\n",
        "Here, we calculate document-level BERTScore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFXUUb3Rwh-U"
      },
      "source": [
        "## 3.0 Installing BERTScore\n",
        "\n",
        "First, we need to install the packages required to compute BERTScore. BERTScore is implemented as part of the bert-score package available in the [official BERTScore GitHub repository](https://github.com/Tiiiger/bert_score). Since embedding-based metrics rely on large embedding models, installing these metrics will usually take considerably longer than installing and importing traditional MT quality scores."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "1XhsQwWMSSXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PsYzOE7zUhR"
      },
      "source": [
        "## 3.1 Calculating an average BERTScore for an entire text\n",
        "\n",
        "We have the machine-translated text file `NMT_hypothesis` and the human-translated reference text `HT_reference`. Simply run the code cell below to calculate an average BERTScore score for the entire text. This may take a little while.\n",
        "\n",
        "Note: In this notebook we use `--lang de`, as the output texts we are working with are German. If you are using texts with a different language, you need to change the language code in the cell below. This step is also shown in our tutorial video."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bert-score -r HT_reference -c NMT_hypothesis --lang de"
      ],
      "metadata": {
        "id": "7IQQhrpYGgpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following three BERTScores should have been calculated:\n",
        "\n",
        "*   Precision `P: 0.874346`\n",
        "*   Recall `R: 0.869231`\n",
        "*   F-Measure `F1: 0.871666`\n",
        "\n",
        "The average BERTScore (F1 score) calculated for the entire *NMT_hypothesis* text is therefore **0.871666**."
      ],
      "metadata": {
        "id": "vyl7N-cfSrJU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfNrPeYnBdj"
      },
      "source": [
        "# 4 COMET\n",
        "\n",
        "Here, we calculate document-level COMET scores."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 Installing COMETinho\n",
        "First, we need to install the packages required to compute COMETinho. COMETinho is implemented as part of the unbabel-comet package available in the [official COMET GitHub repository](https://github.com/Unbabel/COMET).\n",
        "\n",
        "Note: Running this cell might result in requesting you to restart the runtime of this notebook: \"You must restart the runtime in order to use the newly installed version\". You can restart the runtime in the menu under \"runtime\" --> \"restart runtime\". Then, you must run the cell in chapter 0 and the code cell below again before moving on to section 4.1."
      ],
      "metadata": {
        "id": "QM5C-YJwWULU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0XA23gxbCbt"
      },
      "outputs": [],
      "source": [
        "# Install the most recent unbabel-comet package\n",
        "!pip install unbabel-comet==1.1.3\n",
        "\n",
        "# In case this is outdated, check out the newest version or simply uncomment and install the line below\n",
        "#!pip install unbabel-comet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v10ysctSlzd"
      },
      "source": [
        "## 4.1 Calculating an average COMETinho score for an entire text\n",
        "\n",
        "Again we have `NMT_hypothesis` as the machine-translated text and `HT_reference` as the human-translated text. In contrast to the three metrics above, we now also need a source document, so we'll now also indicate the source text `source` in our code cell below.\n",
        "\n",
        "Now we can calculate an average COMETinho score for the entire *NMT_hypthesis* text. We use the same COMETinho Estimator model (`eamt22-cometinho-da`), as in our [Advanced MT Quality Evaluation notebook](https://colab.research.google.com/drive/1UgsqgN-6yfDESU7Geei4RXzJShEQNViZ?usp=sharing). Simply run the code cell below (this may take a while)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgzfvYdKjOQy"
      },
      "outputs": [],
      "source": [
        "# Calculating the COMETinho Score\n",
        "!comet-score -s source -t NMT_hypothesis -r HT_reference --model eamt22-cometinho-da --gpu 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average COMETinho score calculated for the entire *NMT_hypothesis* text is shown at the bottom (*NMT_hypothesis score*) and should be **0.6089**. Above this average score, you can see a individual scores for each segment (each sentence in the text files). Counting starts at 0 and ends at 15, representing the 16 lines in each of our text files. Our sentence COMETinho scores range from 0.0592 to 1.6138."
      ],
      "metadata": {
        "id": "tdl_YwqEHqUC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW4PSxti-m0"
      },
      "source": [
        "## 4.2 Calculating an average COMET Quality Estimation score for an entire text\n",
        "\n",
        "Below, we load the COMET Quality Estimation (QE) model (`wmt21-comet-qe-mqm`) and calculate a quality estimation score is calculated for the hypothesis and source texts (without access to the reference text). Again, calculating this document-level score may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXur6wZSjBLS"
      },
      "outputs": [],
      "source": [
        "# Calculating the Quality Estimation Score\n",
        "!comet-score -s source -t NMT_hypothesis --model wmt21-comet-qe-mqm --gpu 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average COMETinho (QE) score calculated for the entire *NMT_hypothesis* text is **0.1208**. Again, individual segment scores are shown above the average score. The segment scores range from 0.0543 to 1.1589."
      ],
      "metadata": {
        "id": "5RdsISobJSAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now successfully calculated various automatic MT quality evaluation metrics for entire texts (at document level). You are now well prepared to evaluate the quality of MT systems and of machine-translated texts, which is becoming more and more relevant in the professional industry. You could also perform an automatic quality evaluation on your own machine translations created with your own NMT model, which you can train in our [NMT Training notebook](https://colab.research.google.com/drive/1f3V7CshfVvrA5S6XtLAvl-beqBPN3qar?usp=sharing)."
      ],
      "metadata": {
        "id": "MHzcky_CiqDB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}